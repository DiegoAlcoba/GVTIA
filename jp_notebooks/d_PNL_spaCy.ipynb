{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "En este capítulo se verá como procesar lenguaje natural haciendo uso de la librería/herramienta *spaCy*.\n",
    "\n",
    "En lugar de transformar, spaCy conserva el texto original a lo largo de todo el proceso, añadiendo nuevas capas de información al mismo.\n",
    "\n",
    "Se irá mostrando paso a paso como se hace uso de las distintas herramientas que proporciona la librería con ejemplos sencillos, para después finalizar con todo el proceso aplicado sobre el Data Frame que se ha ido creando en los capítulos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gráficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecución\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNL utilizando *spaCy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes de utilizar spaCy\n",
    "A la hora de instalar el modelo el usuario se debe asegurar de que algunas herramientas y librerías estén actualizadas a su última versión y que sean compatibles con la versión de python que se tiene instalada. En mi caso cuento con Python 3.12.7, y he tenido que actualizar con el comando *\"pip install --upgrade nombre-paquete\"*: *pip*, *setuptools*, *wheel*, *spacy*, *pydantic*, *thic*, y asegurarme de que las versiones eran compatibles entre ellas, por ejemplo, para thic tuve que instalar la versión 8.3.0 para asegurar la compatibilidad.\n",
    "\n",
    "Una vez hecho todo esto, ya se puede instalar el modelo desde terminal (en este caso la del entorno virtual de python) con el comando:\n",
    "*python -m spacy download en_core_web_sm*\n",
    "\n",
    "Si aún con todo esto se siguen generando errores, como es mi caso, se recomienda crear un nuevo entorno virtual con una versión de más estable, como la 3.10 o 3.11, para asegurar la compatibilidad.\n",
    "\n",
    "Para poder contar con distintas versiones de Python, es necesario tener instalado el paquete *pyenv* en el sistema. Una vez instalado, es posible instalar la versión deseada, 3.10.9 en mi caso. A partir de este modelo, se creará un entorno virtual utilizando pyenv e indicando la versión que se desea utilizar a la hora de definirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciación de un pipeline\n",
    "Se va a utilizar un modelo de procesamiento preentrenado, para luego instanciar un pipeline que nos servirá a lo largo del capítulo.\n",
    "\n",
    "Cabe recalcar que el modelo utilizado, *en_core_web_sm*, debe ser descargado manualmente antes de utilizarse, en caso contrario no se cargará y el programa lanzará un error.\n",
    "\n",
    "En la variable \"nlp\" se almacenará el objeto *Language*, el cuál contiene el vocabulario, el modelo y el pipeline de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Carga del modelo\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Instanciación del pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador de spaCy es bastante rápido, pero el resto de tareas no. Es por ello que si se desea analizar un dataset considerablemente extenso, se recomienda desactivar algunas funciones del modelo para que la duración disminuya significativamente.\n",
    "\n",
    "En este caso, se va a desactivar *parser* y *named-entity recognition* porque se va a hacer uso, por ahora, del tokenizador y el *part-of-speech tagger*, etiquetador de partes del discurso, el cuál se explicará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x74be38e028c0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x74be3830eda0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x74be38b3dec0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x74be38ea2100>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de texto\n",
    "En la llamada al pipeline nlp, este devuelve un objeto de tipo *spacy.tokens.doc.Doc* que contiene el acceso a los tokens, spans (rangos de tokens), y algunas anotaciones sobre el token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|friend|is|a|professional|race|car|drive|,|I|would|like|to|experience|that|kind|of|feeling|"
     ]
    }
   ],
   "source": [
    "text = \"My friend is a professional race car drive, I would like to experience that kind of feeling\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definirá a continuación una función que genera una tabla que contenga todos los tokens y sus atributos. El resultado se podría definir como un DataFrame en el que se puede utilizar la posición de cada elemento (token) como índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    #Generación del DataFrame con los tokens\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando visualizamos el Data Frame creado, se observa que hay una columna con el atributo \"is_stop\" que indica si se trata de una *stop word* o no sin necesidad de usar un diccionario como se vió en capítulos anteriores gracias al uso del modelo ya preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>professional</td>\n",
       "      <td>professional</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>race</td>\n",
       "      <td>race</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drive</td>\n",
       "      <td>drive</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kind</td>\n",
       "      <td>kind</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>feeling</td>\n",
       "      <td>feel</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text        lemma_  is_stop  is_alpha  pos_ dep_ ent_type_  \\\n",
       "0             My            my     True      True  PRON                  \n",
       "1         friend        friend    False      True  NOUN                  \n",
       "2             is            be     True      True   AUX                  \n",
       "3              a             a     True      True   DET                  \n",
       "4   professional  professional    False      True   ADJ                  \n",
       "5           race          race    False      True  NOUN                  \n",
       "6            car           car    False      True  NOUN                  \n",
       "7          drive         drive    False      True  NOUN                  \n",
       "9              I             I     True      True  PRON                  \n",
       "10         would         would     True      True   AUX                  \n",
       "11          like          like    False      True  VERB                  \n",
       "12            to            to     True      True  PART                  \n",
       "13    experience    experience    False      True  VERB                  \n",
       "14          that          that     True      True   DET                  \n",
       "15          kind          kind    False      True  NOUN                  \n",
       "16            of            of     True      True   ADP                  \n",
       "17       feeling          feel    False      True  VERB                  \n",
       "\n",
       "   ent_iob_  \n",
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5            \n",
       "6            \n",
       "7            \n",
       "9            \n",
       "10           \n",
       "11           \n",
       "12           \n",
       "13           \n",
       "14           \n",
       "15           \n",
       "16           \n",
       "17           "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalización del tokenizador\n",
    "Debido a que la gran mayoría del texto que se va a desear analizar será inglés, hay ciertas palabras o expresiones que deben tomarse en cuenta para que el tokenizador no las elimine o separe. Este es el caso de palabras compuestas que están unidas con guión o guión bajo, expresiones o palabras que se inician con un \"#\" que puede dar un mayor contexto al texto, etc.\n",
    "\n",
    "Se va a definir una función que, haciendo uso del tokenizador de spacy, lo modifica al mismo tiempo para que incluya este tipo de formaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con palabras de parada (Stop Words)\n",
    "Como se vio en la tabla creada antes, spaCy tiene su propio diccionario de Stop Words y clasifica cada token analizado en si se trata de una de esta clase de palabras o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "#Clasificamos como NO stop word aquellas que no lo son (stop words del diccionario y signos de puntuación)\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que, dependiendo del objetivo del trabajo, se quieran incluir o excluir diversas palabras del diccionario en cuestión, por ello se mostrará a continuación un ejemplo de como hacerlo.\n",
    "\n",
    "A partir de la versión 3.0 de spaCy ya no es posible modificar el diccionario de un modelo preentrenado, pero sí se puede crear una subclase para el lenguaje seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan|need|sit|down|talk|Pete|"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "#Palabras que se desean incluir/excluir\n",
    "excluded_stop_words = {'down'}\n",
    "included_stop_words = {'dear', 'regards'}\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults):\n",
    "    #Crea una copia de la lista original de stop words\n",
    "    stop_words = English.Defaults.stop_words.copy()\n",
    "    #Incluye y excluye aquellas que deseamos en la copia que utilizaremos\n",
    "    stop_words -= excluded_stop_words\n",
    "    stop_words |= included_stop_words\n",
    "    \n",
    "class CustomEnglish(English):\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "#utilizamos en el pipeline la nueva lista de palabras creada\n",
    "nlp = CustomEnglish()\n",
    "\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp.make_doc(text) # only tokenize\n",
    "    \n",
    "tokens_wo_stop = [token for token in doc ]\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(token, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos modificar la lista de stop words de forma sencilla en caso de ser necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de lemas basados en partes del discurso\n",
    "El lema de una palabra se refiere a la raíz de la propia palabra. Lematizar un texto puede aumentar la calidad de los modelos al mismo tiempo que ahorrar tiempo y tamaño en el proceso de entrenamiento debido a que el tamaño del vocabulario será menor.\n",
    "\n",
    "En la tabla anterior también se veía la propiedad *\"lemma_\"* de cada token, en este ejemplo se usará una frase cualquiera para extraer los lemas y mostrarlos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "#Se vuelve a cargar el modelo original\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, palabras como \"best\", \"likes\" o \"games\" lo que se muestra es la raíz de la palabra, y no una derivación de la misma, como en este caso pueden ser superlativos o palabras en plural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se hará uso de otro atributo de los tokens, las etiquetas de parte del discurso. Estas etiquetas se pueden definir como abreviaturas del tipo de palabra que es cada token, así \"good\" se mostrará con la etiqueta \"ADJ\", de adjective (adjetivo). \n",
    "\n",
    "Se utilizará el atributo *pos_* el cuál contiene la etiqueta simplificada del token para distintos usos, como en el siguiente, en el que se desea almacenar en una variable \"nouns\" todos los tokens que tenga como tag \"NOUN\" (sustantivos) y \"PROPN\" (sustantivos propios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "#En la variable nouns guardamos aquellos tokens que son sustantivos\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede usar una función de la librería *textacy* que sirve para extraer palabras del texto, sustantivos y adjetivos en este caso, pero funcional con cualqueir tipo de etiqueta. Con esto se consigue, adicionalmente, la posibilidad de filtrar textos con las distintas etiquetas de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc, \n",
    "            filter_stops = True,           # default -> no extrae stop words\n",
    "            filter_punct = True,           # default True -> no extrae signos de puntuación\n",
    "            filter_nums = True,            # default False -> no extrae números\n",
    "            include_pos = ['ADJ', 'NOUN'], # default None -> extraería todas las etiquetas (pos_) \n",
    "            exclude_pos = None,            # default None -> no excluye ninguna\n",
    "            min_freq = 1)                  # frecuencia mínima de palabras\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede definir una función que tiene la misma funcionalidad, incluyendo como parámetro los parámetros clave (**kwargs) que se deben especificar para que el programa sepa qué buscar. Queda a decisión del usuario utilizar una u otra según sus preferencias o necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good|friend|fancy|adventure|game\n"
     ]
    }
   ],
   "source": [
    "#En esta función se extraen los lemas, no los tokens originales con la etiqueta indicada\n",
    "#Extrapolable a la extracción de cualquier tipo de token\n",
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de n-grams\n",
    "En el principio del proyecto se explicó qué eran los n-grams y se explicó que estos no son de mucha utilidad a la hora del análisis, pero eso era porque no se había explicado aún como extraerlos únicamente si tenían un significado real.\n",
    "\n",
    "SpaCy ofrece una potente herramienta basada en reglas, es decir, que un n-gram la mayor parte de las veces tiene significado si las etiquetas (tipos de palabras) que las componen cumplen un cierto criterio y orden, esto junto a la extracción de frases basada en patrones de textacy puede facilitar enormemente esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n"
     ]
    }
   ],
   "source": [
    "#Patrón del n-gram -> adjetivo seguido de uno o más sustantivos\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "\n",
    "#Se tiene en cuenta que puede haber diferentes versiones de textacy\n",
    "if textacy.__version__ < '0.11':\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "else:\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que en el caso anterior, se puede definir una función que realice el mismo trabajo, pero que contará con una mayor flexibilidad al estar hecha a gusto del usuario para cumplir una función específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "#Función que extrae los n-grams indicados\n",
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    if textacy.__version__ < '0.11':\n",
    "        # as in book\n",
    "        spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    else:\n",
    "        # new textacy version\n",
    "        spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de nombres de entidades\n",
    "Las entidades son nombres, generalmente propios, que se refieren a personas, lugares, organizaciones, países, etc. que pueden estar formados por uno o más tipos de palabras.\n",
    "\n",
    "Estas entidades están representadas por objetos *Span* que también cuentan con diversas propiedades.\n",
    "\n",
    "Como se verá a continuación, se pueden extraer estas entidades utilizando directamente *spaCy* o definiendo una función, que también usa la librería, pero es más personalizable, al mismo tiempo que se pueden mostrar por pantalla con distintos formatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impresión por pantalla de las entidades que contiene *doc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impresión de entidades utilizando *displacy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que extrae las entidades de un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"James_O'Neill/PERSON\", 'San_Francisco/GPE']\n"
     ]
    }
   ],
   "source": [
    "print(extract_entities(doc, ['PERSON', 'GPE'])) #GPE: Geopolitical Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características en una única función\n",
    "Ahora se definirá una función que unirá todo lo visto hasta ahora en una única función, la cuál extraerá todos los lemas, adjetivos, entidades, n-grams especificados, etc. del  *doc* que se le pase como argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas'          : extract_lemmas(doc, exclude_pos = ['PART', 'PUNCT', 'DET', 'PRON', 'SYM', 'SPACE'], filter_stops = False),\n",
    "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n",
    "    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilización de spaCy en un Dataset\n",
    "Ahora se mostrará como realizar todas las acciones (tokenizar, extracción de características, etc) vistas hasta el momento sobre un Dataset, en este caso el creado en el anterior capítulo con todos los comentarios de un repositorio de Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexión con la base de datos en la que tenemos guardado el Data Frame\n",
    "db_name = \"../data/zigbee2mqtt_comments.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from comments\", con)\n",
    "con.close()\n",
    "\n",
    "#En cada entrada de la columna texto incluimos el usuario y el comentario que le corresponde\n",
    "#df['text'] = df['user'] + ': ' + df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadiremos en el Data Frame las nuevas columnas que corresponderán con las características extraídas del texto antes de ejecutar ninguna función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemmas', 'adjs_verbs', 'nouns', 'noun_phrases', 'adj_noun_phrases', 'entities']\n"
     ]
    }
   ],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, working on CPU.\n"
     ]
    }
   ],
   "source": [
    "#Indicamos que el programa use la GPU en caso de que se disponga de ella\n",
    "#La ejecución será más rápida que en una CPU\n",
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hará uso del mismo modelo preentrenado, junto con el tokenizador personalizado que se definió al principio del capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "nlp.tokenizer = custom_tokenizer(nlp) #opcional, puede usarse el proporcionado por la librería directamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar grandes datasets es recomendable el uso de procesamiento por lotes para un mayor rendimiento y disminuir el tiempo de ejecución.\n",
    "\n",
    "SpaCy toma el tamaño del lote definido por el usuario, toma el mismo número de textos y los procesa internamente, añadiendo al Doc de forma iterativa los distintos lotes en el mismo orden que los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de extraer las características, crearemos las columnas que almacenará las de cada texto\n",
    "#Crear las columnas basadas en las claves que devuelve extract_nlp\n",
    "sample_doc = next(nlp.pipe(df['text'].iloc[:1]))\n",
    "new_columns = extract_nlp(sample_doc).keys()\n",
    "for col in new_columns:\n",
    "    df[col] = None  # Inicializa las columnas en el DataFrame con valores vacíos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nouns</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Same problem:\\r\\n\\r\\n[13:29:29] INFO: Preparing to start...\\r\\n[13:29:29] INFO: Socat not enabled\\r\\n[13:29:30] INFO: Zigbee Herdsman debug logging enabled\\r\\n[13:29:31] INFO: Starting Zigbee2MQTT...</td>\n",
       "      <td>[same, problem, 13:29:29, info, prepare, start, 13:29:29, info, Socat, enable, 13:29:30, info, Zigbee, Herdsman, debug, log, enable, 13:29:31, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, 2023...</td>\n",
       "      <td>[problem, info, info, Socat, info, Zigbee, Herdsman, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, state, file, state.json, Zigbee2MQTT, info, console, directory, log/2023, filename, log.txt, Z...</td>\n",
       "      <td>[log_directory, 06T10:29:35.446Z_zigbee-herdsman, 06t10:29:35.450z_zigbee-herdsman, 06t10:29:35.463z_zigbee-herdsman, bootloader_payload, 06t10:29:36.467z_zigbee-herdsman, 06t10:29:42.477z_zigbee-...</td>\n",
       "      <td>[Socat/PERSON, Zigbee_Herdsman/PERSON, Zigbee2MQTT_..._\\r\\n/PERSON, Zigbee2MQTT/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PERSON, remove/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>It seems that 1.19.1 might have had some bugs as seen in https://github.com/Koenkk/zigbee2mqtt/releases. Please update to 1.21.0-1 and re-check.</td>\n",
       "      <td>[seem, that, 1.19.1, might, have, have, bug, as, see, in, https://github.com, Koenkk, zigbee2mqtt, release, please, update, to, 1.21.0, 1, and, re-check]</td>\n",
       "      <td>[bug, Koenkk, release, re-check]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days</td>\n",
       "      <td>[issue, be, stale, because, have, be, open, 30, day, with, activity, remove, stale, label, or, comment, or, will, be, close, in, 7, day]</td>\n",
       "      <td>[issue, day, activity, label, comment, day]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>Okay this might actually be a problem of HA and not Z2M.\\r\\nI noticed some strange behaviour in other places under certain conditions too.\\r\\nWill look a bit more into it and probably create an is...</td>\n",
       "      <td>[okay, might, actually, be, problem, of, ha, and, Z2M., notice, strange, behaviour, in, other, place, under, certain, condition, too, will, look, bit, more, into, and, probably, create, issue, wit...</td>\n",
       "      <td>[problem, ha, Z2M., behaviour, place, condition, bit, issue, ha]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ha/ORG, Z2M./ORG, ha/ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>I have exactly the same problem to start with, but in my case the second Zigbee instance does not even appear in the add on list after adding the 1.25-1 repository . The old repository cannot be d...</td>\n",
       "      <td>[have, exactly, same, problem, start, with, but, in, case, second, Zigbee, instance, do, even, appear, in, add, on, list, after, add, 1.25, 1, repository, old, repository, can, be, delete, claim, ...</td>\n",
       "      <td>[problem, case, Zigbee, instance, add, list, repository, repository, use, zigbee2mqtt, host]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[zigbee2mqtt/PERSON]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text  \\\n",
       "834   Same problem:\\r\\n\\r\\n[13:29:29] INFO: Preparing to start...\\r\\n[13:29:29] INFO: Socat not enabled\\r\\n[13:29:30] INFO: Zigbee Herdsman debug logging enabled\\r\\n[13:29:31] INFO: Starting Zigbee2MQTT...   \n",
       "2243                                                        It seems that 1.19.1 might have had some bugs as seen in https://github.com/Koenkk/zigbee2mqtt/releases. Please update to 1.21.0-1 and re-check.    \n",
       "1619                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
       "1268  Okay this might actually be a problem of HA and not Z2M.\\r\\nI noticed some strange behaviour in other places under certain conditions too.\\r\\nWill look a bit more into it and probably create an is...   \n",
       "1734  I have exactly the same problem to start with, but in my case the second Zigbee instance does not even appear in the add on list after adding the 1.25-1 repository . The old repository cannot be d...   \n",
       "\n",
       "                                                                                                                                                                                                       lemmas  \\\n",
       "834   [same, problem, 13:29:29, info, prepare, start, 13:29:29, info, Socat, enable, 13:29:30, info, Zigbee, Herdsman, debug, log, enable, 13:29:31, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, 2023...   \n",
       "2243                                                [seem, that, 1.19.1, might, have, have, bug, as, see, in, https://github.com, Koenkk, zigbee2mqtt, release, please, update, to, 1.21.0, 1, and, re-check]   \n",
       "1619                                                                 [issue, be, stale, because, have, be, open, 30, day, with, activity, remove, stale, label, or, comment, or, will, be, close, in, 7, day]   \n",
       "1268  [okay, might, actually, be, problem, of, ha, and, Z2M., notice, strange, behaviour, in, other, place, under, certain, condition, too, will, look, bit, more, into, and, probably, create, issue, wit...   \n",
       "1734  [have, exactly, same, problem, start, with, but, in, case, second, Zigbee, instance, do, even, appear, in, add, on, list, after, add, 1.25, 1, repository, old, repository, can, be, delete, claim, ...   \n",
       "\n",
       "                                                                                                                                                                                                        nouns  \\\n",
       "834   [problem, info, info, Socat, info, Zigbee, Herdsman, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, state, file, state.json, Zigbee2MQTT, info, console, directory, log/2023, filename, log.txt, Z...   \n",
       "2243                                                                                                                                                                         [bug, Koenkk, release, re-check]   \n",
       "1619                                                                                                                                                              [issue, day, activity, label, comment, day]   \n",
       "1268                                                                                                                                         [problem, ha, Z2M., behaviour, place, condition, bit, issue, ha]   \n",
       "1734                                                                                                             [problem, case, Zigbee, instance, add, list, repository, repository, use, zigbee2mqtt, host]   \n",
       "\n",
       "                                                                                                                                                                                                 noun_phrases  \\\n",
       "834   [log_directory, 06T10:29:35.446Z_zigbee-herdsman, 06t10:29:35.450z_zigbee-herdsman, 06t10:29:35.463z_zigbee-herdsman, bootloader_payload, 06t10:29:36.467z_zigbee-herdsman, 06t10:29:42.477z_zigbee-...   \n",
       "2243                                                                                                                                                                                                       []   \n",
       "1619                                                                                                                                                                                                       []   \n",
       "1268                                                                                                                                                                                                       []   \n",
       "1734                                                                                                                                                                                                       []   \n",
       "\n",
       "                                                                                                                                                                                                     entities  \n",
       "834   [Socat/PERSON, Zigbee_Herdsman/PERSON, Zigbee2MQTT_..._\\r\\n/PERSON, Zigbee2MQTT/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PERSON, remove/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PER...  \n",
       "2243                                                                                                                                                                                                       []  \n",
       "1619                                                                                                                                                                                                       []  \n",
       "1268                                                                                                                                                                               [ha/ORG, Z2M./ORG, ha/ORG]  \n",
       "1734                                                                                                                                                                                     [zigbee2mqtt/PERSON]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'lemmas', 'nouns', 'noun_phrases', 'entities']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado todo el proceso, guardamos el resultado de nuevo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
    "\n",
    "con = sqlite3.connect(db_name) \n",
    "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
