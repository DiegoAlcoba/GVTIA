{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "En este capítulo se verá como procesar lenguaje natural haciendo uso de la librería/herramienta *spaCy*.\n",
    "\n",
    "En lugar de transformar, spaCy conserva el texto original a lo largo de todo el proceso, añadiendo nuevas capas de información al mismo.\n",
    "\n",
    "Se irá mostrando paso a paso como se hace uso de las distintas herramientas que proporciona la librería con ejemplos sencillos, para después finalizar con todo el proceso aplicado sobre el Data Frame que se ha ido creando en los capítulos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gráficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecución\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNL utilizando *spaCy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes de utilizar spaCy\n",
    "A la hora de instalar el modelo el usuario se debe asegurar de que algunas herramientas y librerías estén actualizadas a su última versión y que sean compatibles con la versión de python que se tiene instalada. En mi caso cuento con Python 3.12.7, y he tenido que actualizar con el comando *\"pip install --upgrade nombre-paquete\"*: *pip*, *setuptools*, *wheel*, *spacy*, *pydantic*, *thic*, y asegurarme de que las versiones eran compatibles entre ellas, por ejemplo, para thic tuve que instalar la versión 8.3.0 para asegurar la compatibilidad.\n",
    "\n",
    "Una vez hecho todo esto, ya se puede instalar el modelo desde terminal (en este caso la del entorno virtual de python) con el comando:\n",
    "*python -m spacy download en_core_web_sm*\n",
    "\n",
    "Si aún con todo esto se siguen generando errores, como es mi caso, se recomienda crear un nuevo entorno virtual con una versión de más estable, como la 3.10 o 3.11, para asegurar la compatibilidad.\n",
    "\n",
    "Para poder contar con distintas versiones de Python, es necesario tener instalado el paquete *pyenv* en el sistema. Una vez instalado, es posible instalar la versión deseada, 3.10.9 en mi caso. A partir de este modelo, se creará un entorno virtual utilizando pyenv e indicando la versión que se desea utilizar a la hora de definirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciación de un pipeline\n",
    "Se va a utilizar un modelo de procesamiento preentrenado, para luego instanciar un pipeline que nos servirá a lo largo del capítulo.\n",
    "\n",
    "Cabe recalcar que el modelo utilizado, *en_core_web_sm*, debe ser descargado manualmente antes de utilizarse, en caso contrario no se cargará y el programa lanzará un error.\n",
    "\n",
    "En la variable \"nlp\" se almacenará el objeto *Language*, el cuál contiene el vocabulario, el modelo y el pipeline de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x70c8b4a5e080>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x70c8b4a5e680>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x70c8b4cb7ed0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x70c8b49a03c0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x70c8b487d840>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x70c8b4cb7df0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Carga del modelo\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Instanciación del pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador de spaCy es bastante rápido, pero el resto de tareas no. Es por ello que si se desea analizar un dataset considerablemente extenso, se recomienda desactivar algunas funciones del modelo para que la duración disminuya significativamente.\n",
    "\n",
    "En este caso, se va a desactivar *parser* y *named-entity recognition* porque se va a hacer uso, por ahora, del tokenizador y el *part-of-speech tagger*, etiquetador de partes del discurso, el cuál se explicará más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x70c8b3f867a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x70c8b48b47c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x70c8b3f06bc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x70c8b3f06b00>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de texto\n",
    "En la llamada al pipeline nlp, este devuelve un objeto de tipo *spacy.tokens.doc.Doc* que contiene el acceso a los tokens, spans (rangos de tokens), y algunas anotaciones sobre el token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|friend|is|a|professional|race|car|drive|,|I|would|like|to|experience|that|kind|of|feeling|"
     ]
    }
   ],
   "source": [
    "text = \"My friend is a professional race car drive, I would like to experience that kind of feeling\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definirá a continuación una función que genera una tabla que contenga todos los tokens y sus atributos. El resultado se podría definir como un DataFrame en el que se puede utilizar la posición de cada elemento (token) como índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    #Generación del DataFrame con los tokens\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando visualizamos el Data Frame creado, se observa que hay una columna con el atributo \"is_stop\" que indica si se trata de una *stop word* o no sin necesidad de usar un diccionario como se vió en capítulos anteriores gracias al uso del modelo ya preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>professional</td>\n",
       "      <td>professional</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>race</td>\n",
       "      <td>race</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drive</td>\n",
       "      <td>drive</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kind</td>\n",
       "      <td>kind</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>feeling</td>\n",
       "      <td>feel</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text        lemma_  is_stop  is_alpha  pos_ dep_ ent_type_  \\\n",
       "0             My            my     True      True  PRON                  \n",
       "1         friend        friend    False      True  NOUN                  \n",
       "2             is            be     True      True   AUX                  \n",
       "3              a             a     True      True   DET                  \n",
       "4   professional  professional    False      True   ADJ                  \n",
       "5           race          race    False      True  NOUN                  \n",
       "6            car           car    False      True  NOUN                  \n",
       "7          drive         drive    False      True  NOUN                  \n",
       "9              I             I     True      True  PRON                  \n",
       "10         would         would     True      True   AUX                  \n",
       "11          like          like    False      True  VERB                  \n",
       "12            to            to     True      True  PART                  \n",
       "13    experience    experience    False      True  VERB                  \n",
       "14          that          that     True      True   DET                  \n",
       "15          kind          kind    False      True  NOUN                  \n",
       "16            of            of     True      True   ADP                  \n",
       "17       feeling          feel    False      True  VERB                  \n",
       "\n",
       "   ent_iob_  \n",
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5            \n",
       "6            \n",
       "7            \n",
       "9            \n",
       "10           \n",
       "11           \n",
       "12           \n",
       "13           \n",
       "14           \n",
       "15           \n",
       "16           \n",
       "17           "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalización del tokenizador\n",
    "Debido a que la gran mayoría del texto que se va a desear analizar será inglés, hay ciertas palabras o expresiones que deben tomarse en cuenta para que el tokenizador no las elimine o separe. Este es el caso de palabras compuestas que están unidas con guión o guión bajo, expresiones o palabras que se inician con un \"#\" que puede dar un mayor contexto al texto, etc.\n",
    "\n",
    "Se va a definir una función que, haciendo uso del tokenizador de spacy, lo modifica al mismo tiempo para que incluya este tipo de formaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
