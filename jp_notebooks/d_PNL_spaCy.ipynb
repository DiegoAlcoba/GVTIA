{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "En este cap칤tulo se ver치 como procesar lenguaje natural haciendo uso de la librer칤a/herramienta *spaCy*.\n",
    "\n",
    "En lugar de transformar, spaCy conserva el texto original a lo largo de todo el proceso, a침adiendo nuevas capas de informaci칩n al mismo.\n",
    "\n",
    "Se ir치 mostrando paso a paso como se hace uso de las distintas herramientas que proporciona la librer칤a con ejemplos sencillos, para despu칠s finalizar con todo el proceso aplicado sobre el Data Frame que se ha ido creando en los cap칤tulos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gr치ficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecuci칩n\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNL utilizando *spaCy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes de utilizar spaCy\n",
    "A la hora de instalar el modelo el usuario se debe asegurar de que algunas herramientas y librer칤as est칠n actualizadas a su 칰ltima versi칩n y que sean compatibles con la versi칩n de python que se tiene instalada. En mi caso cuento con Python 3.12.7, y he tenido que actualizar con el comando *\"pip install --upgrade nombre-paquete\"*: *pip*, *setuptools*, *wheel*, *spacy*, *pydantic*, *thic*, y asegurarme de que las versiones eran compatibles entre ellas, por ejemplo, para thic tuve que instalar la versi칩n 8.3.0 para asegurar la compatibilidad.\n",
    "\n",
    "Una vez hecho todo esto, ya se puede instalar el modelo desde terminal (en este caso la del entorno virtual de python) con el comando:\n",
    "*python -m spacy download en_core_web_sm*\n",
    "\n",
    "Si a칰n con todo esto se siguen generando errores, como es mi caso, se recomienda crear un nuevo entorno virtual con una versi칩n de m치s estable, como la 3.10 o 3.11, para asegurar la compatibilidad.\n",
    "\n",
    "Para poder contar con distintas versiones de Python, es necesario tener instalado el paquete *pyenv* en el sistema. Una vez instalado, es posible instalar la versi칩n deseada, 3.10.9 en mi caso. A partir de este modelo, se crear치 un entorno virtual utilizando pyenv e indicando la versi칩n que se desea utilizar a la hora de definirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciaci칩n de un pipeline\n",
    "Se va a utilizar un modelo de procesamiento preentrenado, para luego instanciar un pipeline que nos servir치 a lo largo del cap칤tulo.\n",
    "\n",
    "Cabe recalcar que el modelo utilizado, *en_core_web_sm*, debe ser descargado manualmente antes de utilizarse, en caso contrario no se cargar치 y el programa lanzar치 un error.\n",
    "\n",
    "En la variable \"nlp\" se almacenar치 el objeto *Language*, el cu치l contiene el vocabulario, el modelo y el pipeline de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x70c8b4a5e080>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x70c8b4a5e680>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x70c8b4cb7ed0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x70c8b49a03c0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x70c8b487d840>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x70c8b4cb7df0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Carga del modelo\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Instanciaci칩n del pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador de spaCy es bastante r치pido, pero el resto de tareas no. Es por ello que si se desea analizar un dataset considerablemente extenso, se recomienda desactivar algunas funciones del modelo para que la duraci칩n disminuya significativamente.\n",
    "\n",
    "En este caso, se va a desactivar *parser* y *named-entity recognition* porque se va a hacer uso, por ahora, del tokenizador y el *part-of-speech tagger*, etiquetador de partes del discurso, el cu치l se explicar치 m치s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x70c8b3f867a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x70c8b48b47c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x70c8b3f06bc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x70c8b3f06b00>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de texto\n",
    "En la llamada al pipeline nlp, este devuelve un objeto de tipo *spacy.tokens.doc.Doc* que contiene el acceso a los tokens, spans (rangos de tokens), y algunas anotaciones sobre el token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|friend|is|a|professional|race|car|drive|,|I|would|like|to|experience|that|kind|of|feeling|"
     ]
    }
   ],
   "source": [
    "text = \"My friend is a professional race car drive, I would like to experience that kind of feeling\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definir치 a continuaci칩n una funci칩n que genera una tabla que contenga todos los tokens y sus atributos. El resultado se podr칤a definir como un DataFrame en el que se puede utilizar la posici칩n de cada elemento (token) como 칤ndice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    #Generaci칩n del DataFrame con los tokens\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando visualizamos el Data Frame creado, se observa que hay una columna con el atributo \"is_stop\" que indica si se trata de una *stop word* o no sin necesidad de usar un diccionario como se vi칩 en cap칤tulos anteriores gracias al uso del modelo ya preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>professional</td>\n",
       "      <td>professional</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>race</td>\n",
       "      <td>race</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drive</td>\n",
       "      <td>drive</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kind</td>\n",
       "      <td>kind</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>feeling</td>\n",
       "      <td>feel</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text        lemma_  is_stop  is_alpha  pos_ dep_ ent_type_  \\\n",
       "0             My            my     True      True  PRON                  \n",
       "1         friend        friend    False      True  NOUN                  \n",
       "2             is            be     True      True   AUX                  \n",
       "3              a             a     True      True   DET                  \n",
       "4   professional  professional    False      True   ADJ                  \n",
       "5           race          race    False      True  NOUN                  \n",
       "6            car           car    False      True  NOUN                  \n",
       "7          drive         drive    False      True  NOUN                  \n",
       "9              I             I     True      True  PRON                  \n",
       "10         would         would     True      True   AUX                  \n",
       "11          like          like    False      True  VERB                  \n",
       "12            to            to     True      True  PART                  \n",
       "13    experience    experience    False      True  VERB                  \n",
       "14          that          that     True      True   DET                  \n",
       "15          kind          kind    False      True  NOUN                  \n",
       "16            of            of     True      True   ADP                  \n",
       "17       feeling          feel    False      True  VERB                  \n",
       "\n",
       "   ent_iob_  \n",
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5            \n",
       "6            \n",
       "7            \n",
       "9            \n",
       "10           \n",
       "11           \n",
       "12           \n",
       "13           \n",
       "14           \n",
       "15           \n",
       "16           \n",
       "17           "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizaci칩n del tokenizador\n",
    "Debido a que la gran mayor칤a del texto que se va a desear analizar ser치 ingl칠s, hay ciertas palabras o expresiones que deben tomarse en cuenta para que el tokenizador no las elimine o separe. Este es el caso de palabras compuestas que est치n unidas con gui칩n o gui칩n bajo, expresiones o palabras que se inician con un \"#\" que puede dar un mayor contexto al texto, etc.\n",
    "\n",
    "Se va a definir una funci칩n que, haciendo uso del tokenizador de spacy, lo modifica al mismo tiempo para que incluya este tipo de formaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|游땖|游녨|"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 游땖游녨\"\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
