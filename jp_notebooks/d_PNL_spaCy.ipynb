{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "En este cap칤tulo se ver치 como procesar lenguaje natural haciendo uso de la librer칤a/herramienta *spaCy*.\n",
    "\n",
    "En lugar de transformar, spaCy conserva el texto original a lo largo de todo el proceso, a침adiendo nuevas capas de informaci칩n al mismo.\n",
    "\n",
    "Se ir치 mostrando paso a paso como se hace uso de las distintas herramientas que proporciona la librer칤a con ejemplos sencillos, para despu칠s finalizar con todo el proceso aplicado sobre el Data Frame que se ha ido creando en los cap칤tulos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gr치ficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecuci칩n\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNL utilizando *spaCy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes de utilizar spaCy\n",
    "A la hora de instalar el modelo el usuario se debe asegurar de que algunas herramientas y librer칤as est칠n actualizadas a su 칰ltima versi칩n y que sean compatibles con la versi칩n de python que se tiene instalada. En mi caso cuento con Python 3.12.7, y he tenido que actualizar con el comando *\"pip install --upgrade nombre-paquete\"*: *pip*, *setuptools*, *wheel*, *spacy*, *pydantic*, *thic*, y asegurarme de que las versiones eran compatibles entre ellas, por ejemplo, para thic tuve que instalar la versi칩n 8.3.0 para asegurar la compatibilidad.\n",
    "\n",
    "Una vez hecho todo esto, ya se puede instalar el modelo desde terminal (en este caso la del entorno virtual de python) con el comando:\n",
    "*python -m spacy download en_core_web_sm*\n",
    "\n",
    "Si a칰n con todo esto se siguen generando errores, como es mi caso, se recomienda crear un nuevo entorno virtual con una versi칩n de m치s estable, como la 3.10 o 3.11, para asegurar la compatibilidad.\n",
    "\n",
    "Para poder contar con distintas versiones de Python, es necesario tener instalado el paquete *pyenv* en el sistema. Una vez instalado, es posible instalar la versi칩n deseada, 3.10.9 en mi caso. A partir de este modelo, se crear치 un entorno virtual utilizando pyenv e indicando la versi칩n que se desea utilizar a la hora de definirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciaci칩n de un pipeline\n",
    "Se va a utilizar un modelo de procesamiento preentrenado, para luego instanciar un pipeline que nos servir치 a lo largo del cap칤tulo.\n",
    "\n",
    "Cabe recalcar que el modelo utilizado, *en_core_web_sm*, debe ser descargado manualmente antes de utilizarse, en caso contrario no se cargar치 y el programa lanzar치 un error.\n",
    "\n",
    "En la variable \"nlp\" se almacenar치 el objeto *Language*, el cu치l contiene el vocabulario, el modelo y el pipeline de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Carga del modelo\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Instanciaci칩n del pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador de spaCy es bastante r치pido, pero el resto de tareas no. Es por ello que si se desea analizar un dataset considerablemente extenso, se recomienda desactivar algunas funciones del modelo para que la duraci칩n disminuya significativamente.\n",
    "\n",
    "En este caso, se va a desactivar *parser* y *named-entity recognition* porque se va a hacer uso, por ahora, del tokenizador y el *part-of-speech tagger*, etiquetador de partes del discurso, el cu치l se explicar치 m치s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x74be38e028c0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x74be3830eda0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x74be38b3dec0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x74be38ea2100>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de texto\n",
    "En la llamada al pipeline nlp, este devuelve un objeto de tipo *spacy.tokens.doc.Doc* que contiene el acceso a los tokens, spans (rangos de tokens), y algunas anotaciones sobre el token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|friend|is|a|professional|race|car|drive|,|I|would|like|to|experience|that|kind|of|feeling|"
     ]
    }
   ],
   "source": [
    "text = \"My friend is a professional race car drive, I would like to experience that kind of feeling\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definir치 a continuaci칩n una funci칩n que genera una tabla que contenga todos los tokens y sus atributos. El resultado se podr칤a definir como un DataFrame en el que se puede utilizar la posici칩n de cada elemento (token) como 칤ndice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    #Generaci칩n del DataFrame con los tokens\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando visualizamos el Data Frame creado, se observa que hay una columna con el atributo \"is_stop\" que indica si se trata de una *stop word* o no sin necesidad de usar un diccionario como se vi칩 en cap칤tulos anteriores gracias al uso del modelo ya preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>professional</td>\n",
       "      <td>professional</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>race</td>\n",
       "      <td>race</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drive</td>\n",
       "      <td>drive</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kind</td>\n",
       "      <td>kind</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>feeling</td>\n",
       "      <td>feel</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text        lemma_  is_stop  is_alpha  pos_ dep_ ent_type_  \\\n",
       "0             My            my     True      True  PRON                  \n",
       "1         friend        friend    False      True  NOUN                  \n",
       "2             is            be     True      True   AUX                  \n",
       "3              a             a     True      True   DET                  \n",
       "4   professional  professional    False      True   ADJ                  \n",
       "5           race          race    False      True  NOUN                  \n",
       "6            car           car    False      True  NOUN                  \n",
       "7          drive         drive    False      True  NOUN                  \n",
       "9              I             I     True      True  PRON                  \n",
       "10         would         would     True      True   AUX                  \n",
       "11          like          like    False      True  VERB                  \n",
       "12            to            to     True      True  PART                  \n",
       "13    experience    experience    False      True  VERB                  \n",
       "14          that          that     True      True   DET                  \n",
       "15          kind          kind    False      True  NOUN                  \n",
       "16            of            of     True      True   ADP                  \n",
       "17       feeling          feel    False      True  VERB                  \n",
       "\n",
       "   ent_iob_  \n",
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5            \n",
       "6            \n",
       "7            \n",
       "9            \n",
       "10           \n",
       "11           \n",
       "12           \n",
       "13           \n",
       "14           \n",
       "15           \n",
       "16           \n",
       "17           "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizaci칩n del tokenizador\n",
    "Debido a que la gran mayor칤a del texto que se va a desear analizar ser치 ingl칠s, hay ciertas palabras o expresiones que deben tomarse en cuenta para que el tokenizador no las elimine o separe. Este es el caso de palabras compuestas que est치n unidas con gui칩n o gui칩n bajo, expresiones o palabras que se inician con un \"#\" que puede dar un mayor contexto al texto, etc.\n",
    "\n",
    "Se va a definir una funci칩n que, haciendo uso del tokenizador de spacy, lo modifica al mismo tiempo para que incluya este tipo de formaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|游땖|游녨|"
     ]
    }
   ],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 游땖游녨\"\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con palabras de parada (Stop Words)\n",
    "Como se vio en la tabla creada antes, spaCy tiene su propio diccionario de Stop Words y clasifica cada token analizado en si se trata de una de esta clase de palabras o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "#Clasificamos como NO stop word aquellas que no lo son (stop words del diccionario y signos de puntuaci칩n)\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que, dependiendo del objetivo del trabajo, se quieran incluir o excluir diversas palabras del diccionario en cuesti칩n, por ello se mostrar치 a continuaci칩n un ejemplo de como hacerlo.\n",
    "\n",
    "A partir de la versi칩n 3.0 de spaCy ya no es posible modificar el diccionario de un modelo preentrenado, pero s칤 se puede crear una subclase para el lenguaje seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan|need|sit|down|talk|Pete|"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "#Palabras que se desean incluir/excluir\n",
    "excluded_stop_words = {'down'}\n",
    "included_stop_words = {'dear', 'regards'}\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults):\n",
    "    #Crea una copia de la lista original de stop words\n",
    "    stop_words = English.Defaults.stop_words.copy()\n",
    "    #Incluye y excluye aquellas que deseamos en la copia que utilizaremos\n",
    "    stop_words -= excluded_stop_words\n",
    "    stop_words |= included_stop_words\n",
    "    \n",
    "class CustomEnglish(English):\n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "#utilizamos en el pipeline la nueva lista de palabras creada\n",
    "nlp = CustomEnglish()\n",
    "\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp.make_doc(text) # only tokenize\n",
    "    \n",
    "tokens_wo_stop = [token for token in doc ]\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(token, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos modificar la lista de stop words de forma sencilla en caso de ser necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci칩n de lemas basados en partes del discurso\n",
    "El lema de una palabra se refiere a la ra칤z de la propia palabra. Lematizar un texto puede aumentar la calidad de los modelos al mismo tiempo que ahorrar tiempo y tama침o en el proceso de entrenamiento debido a que el tama침o del vocabulario ser치 menor.\n",
    "\n",
    "En la tabla anterior tambi칠n se ve칤a la propiedad *\"lemma_\"* de cada token, en este ejemplo se usar치 una frase cualquiera para extraer los lemas y mostrarlos por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "#Se vuelve a cargar el modelo original\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, palabras como \"best\", \"likes\" o \"games\" lo que se muestra es la ra칤z de la palabra, y no una derivaci칩n de la misma, como en este caso pueden ser superlativos o palabras en plural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi칠n se har치 uso de otro atributo de los tokens, las etiquetas de parte del discurso. Estas etiquetas se pueden definir como abreviaturas del tipo de palabra que es cada token, as칤 \"good\" se mostrar치 con la etiqueta \"ADJ\", de adjective (adjetivo). \n",
    "\n",
    "Se utilizar치 el atributo *pos_* el cu치l contiene la etiqueta simplificada del token para distintos usos, como en el siguiente, en el que se desea almacenar en una variable \"nouns\" todos los tokens que tenga como tag \"NOUN\" (sustantivos) y \"PROPN\" (sustantivos propios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "#En la variable nouns guardamos aquellos tokens que son sustantivos\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede usar una funci칩n de la librer칤a *textacy* que sirve para extraer palabras del texto, sustantivos y adjetivos en este caso, pero funcional con cualqueir tipo de etiqueta. Con esto se consigue, adicionalmente, la posibilidad de filtrar textos con las distintas etiquetas de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc, \n",
    "            filter_stops = True,           # default -> no extrae stop words\n",
    "            filter_punct = True,           # default True -> no extrae signos de puntuaci칩n\n",
    "            filter_nums = True,            # default False -> no extrae n칰meros\n",
    "            include_pos = ['ADJ', 'NOUN'], # default None -> extraer칤a todas las etiquetas (pos_) \n",
    "            exclude_pos = None,            # default None -> no excluye ninguna\n",
    "            min_freq = 1)                  # frecuencia m칤nima de palabras\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi칠n se puede definir una funci칩n que tiene la misma funcionalidad, incluyendo como par치metro los par치metros clave (**kwargs) que se deben especificar para que el programa sepa qu칠 buscar. Queda a decisi칩n del usuario utilizar una u otra seg칰n sus preferencias o necesidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good|friend|fancy|adventure|game\n"
     ]
    }
   ],
   "source": [
    "#En esta funci칩n se extraen los lemas, no los tokens originales con la etiqueta indicada\n",
    "#Extrapolable a la extracci칩n de cualquier tipo de token\n",
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci칩n de n-grams\n",
    "En el principio del proyecto se explic칩 qu칠 eran los n-grams y se explic칩 que estos no son de mucha utilidad a la hora del an치lisis, pero eso era porque no se hab칤a explicado a칰n como extraerlos 칰nicamente si ten칤an un significado real.\n",
    "\n",
    "SpaCy ofrece una potente herramienta basada en reglas, es decir, que un n-gram la mayor parte de las veces tiene significado si las etiquetas (tipos de palabras) que las componen cumplen un cierto criterio y orden, esto junto a la extracci칩n de frases basada en patrones de textacy puede facilitar enormemente esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n"
     ]
    }
   ],
   "source": [
    "#Patr칩n del n-gram -> adjetivo seguido de uno o m치s sustantivos\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "\n",
    "#Se tiene en cuenta que puede haber diferentes versiones de textacy\n",
    "if textacy.__version__ < '0.11':\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "else:\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que en el caso anterior, se puede definir una funci칩n que realice el mismo trabajo, pero que contar치 con una mayor flexibilidad al estar hecha a gusto del usuario para cumplir una funci칩n espec칤fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "#Funci칩n que extrae los n-grams indicados\n",
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    if textacy.__version__ < '0.11':\n",
    "        # as in book\n",
    "        spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    else:\n",
    "        # new textacy version\n",
    "        spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci칩n de nombres de entidades\n",
    "Las entidades son nombres, generalmente propios, que se refieren a personas, lugares, organizaciones, pa칤ses, etc. que pueden estar formados por uno o m치s tipos de palabras.\n",
    "\n",
    "Estas entidades est치n representadas por objetos *Span* que tambi칠n cuentan con diversas propiedades.\n",
    "\n",
    "Como se ver치 a continuaci칩n, se pueden extraer estas entidades utilizando directamente *spaCy* o definiendo una funci칩n, que tambi칠n usa la librer칤a, pero es m치s personalizable, al mismo tiempo que se pueden mostrar por pantalla con distintos formatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impresi칩n por pantalla de las entidades que contiene *doc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impresi칩n de entidades utilizando *displacy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funci칩n que extrae las entidades de un documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"James_O'Neill/PERSON\", 'San_Francisco/GPE']\n"
     ]
    }
   ],
   "source": [
    "print(extract_entities(doc, ['PERSON', 'GPE'])) #GPE: Geopolitical Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci칩n de caracter칤sticas en una 칰nica funci칩n\n",
    "Ahora se definir치 una funci칩n que unir치 todo lo visto hasta ahora en una 칰nica funci칩n, la cu치l extraer치 todos los lemas, adjetivos, entidades, n-grams especificados, etc. del  *doc* que se le pase como argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas'          : extract_lemmas(doc, exclude_pos = ['PART', 'PUNCT', 'DET', 'PRON', 'SYM', 'SPACE'], filter_stops = False),\n",
    "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n",
    "    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizaci칩n de spaCy en un Dataset\n",
    "Ahora se mostrar치 como realizar todas las acciones (tokenizar, extracci칩n de caracter칤sticas, etc) vistas hasta el momento sobre un Dataset, en este caso el creado en el anterior cap칤tulo con todos los comentarios de un repositorio de Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexi칩n con la base de datos en la que tenemos guardado el Data Frame\n",
    "db_name = \"../data/zigbee2mqtt_comments.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from comments\", con)\n",
    "con.close()\n",
    "\n",
    "#En cada entrada de la columna texto incluimos el usuario y el comentario que le corresponde\n",
    "#df['text'] = df['user'] + ': ' + df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A침adiremos en el Data Frame las nuevas columnas que corresponder치n con las caracter칤sticas extra칤das del texto antes de ejecutar ninguna funci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemmas', 'adjs_verbs', 'nouns', 'noun_phrases', 'adj_noun_phrases', 'entities']\n"
     ]
    }
   ],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, working on CPU.\n"
     ]
    }
   ],
   "source": [
    "#Indicamos que el programa use la GPU en caso de que se disponga de ella\n",
    "#La ejecuci칩n ser치 m치s r치pida que en una CPU\n",
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se har치 uso del mismo modelo preentrenado, junto con el tokenizador personalizado que se defini칩 al principio del cap칤tulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "nlp.tokenizer = custom_tokenizer(nlp) #opcional, puede usarse el proporcionado por la librer칤a directamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar grandes datasets es recomendable el uso de procesamiento por lotes para un mayor rendimiento y disminuir el tiempo de ejecuci칩n.\n",
    "\n",
    "SpaCy toma el tama침o del lote definido por el usuario, toma el mismo n칰mero de textos y los procesa internamente, a침adiendo al Doc de forma iterativa los distintos lotes en el mismo orden que los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de extraer las caracter칤sticas, crearemos las columnas que almacenar치 las de cada texto\n",
    "#Crear las columnas basadas en las claves que devuelve extract_nlp\n",
    "sample_doc = next(nlp.pipe(df['text'].iloc[:1]))\n",
    "new_columns = extract_nlp(sample_doc).keys()\n",
    "for col in new_columns:\n",
    "    df[col] = None  # Inicializa las columnas en el DataFrame con valores vac칤os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nouns</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Same problem:\\r\\n\\r\\n[13:29:29] INFO: Preparing to start...\\r\\n[13:29:29] INFO: Socat not enabled\\r\\n[13:29:30] INFO: Zigbee Herdsman debug logging enabled\\r\\n[13:29:31] INFO: Starting Zigbee2MQTT...</td>\n",
       "      <td>[same, problem, 13:29:29, info, prepare, start, 13:29:29, info, Socat, enable, 13:29:30, info, Zigbee, Herdsman, debug, log, enable, 13:29:31, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, 2023...</td>\n",
       "      <td>[problem, info, info, Socat, info, Zigbee, Herdsman, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, state, file, state.json, Zigbee2MQTT, info, console, directory, log/2023, filename, log.txt, Z...</td>\n",
       "      <td>[log_directory, 06T10:29:35.446Z_zigbee-herdsman, 06t10:29:35.450z_zigbee-herdsman, 06t10:29:35.463z_zigbee-herdsman, bootloader_payload, 06t10:29:36.467z_zigbee-herdsman, 06t10:29:42.477z_zigbee-...</td>\n",
       "      <td>[Socat/PERSON, Zigbee_Herdsman/PERSON, Zigbee2MQTT_..._\\r\\n/PERSON, Zigbee2MQTT/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PERSON, remove/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>It seems that 1.19.1 might have had some bugs as seen in https://github.com/Koenkk/zigbee2mqtt/releases. Please update to 1.21.0-1 and re-check.</td>\n",
       "      <td>[seem, that, 1.19.1, might, have, have, bug, as, see, in, https://github.com, Koenkk, zigbee2mqtt, release, please, update, to, 1.21.0, 1, and, re-check]</td>\n",
       "      <td>[bug, Koenkk, release, re-check]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days</td>\n",
       "      <td>[issue, be, stale, because, have, be, open, 30, day, with, activity, remove, stale, label, or, comment, or, will, be, close, in, 7, day]</td>\n",
       "      <td>[issue, day, activity, label, comment, day]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>Okay this might actually be a problem of HA and not Z2M.\\r\\nI noticed some strange behaviour in other places under certain conditions too.\\r\\nWill look a bit more into it and probably create an is...</td>\n",
       "      <td>[okay, might, actually, be, problem, of, ha, and, Z2M., notice, strange, behaviour, in, other, place, under, certain, condition, too, will, look, bit, more, into, and, probably, create, issue, wit...</td>\n",
       "      <td>[problem, ha, Z2M., behaviour, place, condition, bit, issue, ha]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ha/ORG, Z2M./ORG, ha/ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>I have exactly the same problem to start with, but in my case the second Zigbee instance does not even appear in the add on list after adding the 1.25-1 repository . The old repository cannot be d...</td>\n",
       "      <td>[have, exactly, same, problem, start, with, but, in, case, second, Zigbee, instance, do, even, appear, in, add, on, list, after, add, 1.25, 1, repository, old, repository, can, be, delete, claim, ...</td>\n",
       "      <td>[problem, case, Zigbee, instance, add, list, repository, repository, use, zigbee2mqtt, host]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[zigbee2mqtt/PERSON]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text  \\\n",
       "834   Same problem:\\r\\n\\r\\n[13:29:29] INFO: Preparing to start...\\r\\n[13:29:29] INFO: Socat not enabled\\r\\n[13:29:30] INFO: Zigbee Herdsman debug logging enabled\\r\\n[13:29:31] INFO: Starting Zigbee2MQTT...   \n",
       "2243                                                        It seems that 1.19.1 might have had some bugs as seen in https://github.com/Koenkk/zigbee2mqtt/releases. Please update to 1.21.0-1 and re-check.    \n",
       "1619                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
       "1268  Okay this might actually be a problem of HA and not Z2M.\\r\\nI noticed some strange behaviour in other places under certain conditions too.\\r\\nWill look a bit more into it and probably create an is...   \n",
       "1734  I have exactly the same problem to start with, but in my case the second Zigbee instance does not even appear in the add on list after adding the 1.25-1 repository . The old repository cannot be d...   \n",
       "\n",
       "                                                                                                                                                                                                       lemmas  \\\n",
       "834   [same, problem, 13:29:29, info, prepare, start, 13:29:29, info, Socat, enable, 13:29:30, info, Zigbee, Herdsman, debug, log, enable, 13:29:31, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, 2023...   \n",
       "2243                                                [seem, that, 1.19.1, might, have, have, bug, as, see, in, https://github.com, Koenkk, zigbee2mqtt, release, please, update, to, 1.21.0, 1, and, re-check]   \n",
       "1619                                                                 [issue, be, stale, because, have, be, open, 30, day, with, activity, remove, stale, label, or, comment, or, will, be, close, in, 7, day]   \n",
       "1268  [okay, might, actually, be, problem, of, ha, and, Z2M., notice, strange, behaviour, in, other, place, under, certain, condition, too, will, look, bit, more, into, and, probably, create, issue, wit...   \n",
       "1734  [have, exactly, same, problem, start, with, but, in, case, second, Zigbee, instance, do, even, appear, in, add, on, list, after, add, 1.25, 1, repository, old, repository, can, be, delete, claim, ...   \n",
       "\n",
       "                                                                                                                                                                                                        nouns  \\\n",
       "834   [problem, info, info, Socat, info, Zigbee, Herdsman, info, Starting, Zigbee2MQTT, Zigbee2MQTT, debug, state, file, state.json, Zigbee2MQTT, info, console, directory, log/2023, filename, log.txt, Z...   \n",
       "2243                                                                                                                                                                         [bug, Koenkk, release, re-check]   \n",
       "1619                                                                                                                                                              [issue, day, activity, label, comment, day]   \n",
       "1268                                                                                                                                         [problem, ha, Z2M., behaviour, place, condition, bit, issue, ha]   \n",
       "1734                                                                                                             [problem, case, Zigbee, instance, add, list, repository, repository, use, zigbee2mqtt, host]   \n",
       "\n",
       "                                                                                                                                                                                                 noun_phrases  \\\n",
       "834   [log_directory, 06T10:29:35.446Z_zigbee-herdsman, 06t10:29:35.450z_zigbee-herdsman, 06t10:29:35.463z_zigbee-herdsman, bootloader_payload, 06t10:29:36.467z_zigbee-herdsman, 06t10:29:42.477z_zigbee-...   \n",
       "2243                                                                                                                                                                                                       []   \n",
       "1619                                                                                                                                                                                                       []   \n",
       "1268                                                                                                                                                                                                       []   \n",
       "1734                                                                                                                                                                                                       []   \n",
       "\n",
       "                                                                                                                                                                                                     entities  \n",
       "834   [Socat/PERSON, Zigbee_Herdsman/PERSON, Zigbee2MQTT_..._\\r\\n/PERSON, Zigbee2MQTT/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PERSON, remove/PERSON, Zigbee2MQTT_:_info_ /PERSON, Zigbee2MQTT/PER...  \n",
       "2243                                                                                                                                                                                                       []  \n",
       "1619                                                                                                                                                                                                       []  \n",
       "1268                                                                                                                                                                               [ha/ORG, Z2M./ORG, ha/ORG]  \n",
       "1734                                                                                                                                                                                     [zigbee2mqtt/PERSON]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'lemmas', 'nouns', 'noun_phrases', 'entities']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado todo el proceso, guardamos el resultado de nuevo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
    "\n",
    "con = sqlite3.connect(db_name) \n",
    "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
