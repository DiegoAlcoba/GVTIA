{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relaciones semánticas con Word Embeddings\n",
    "Anteriormente se ha visto cómo se puede calcular la similitud entre textos a través de la comparación e identificando tokens comunes entre ellos, pero es muy posible que debido a la complejidad del lenguaje, dos textos tengan un significado similar sin tener en común ninguna palabra.\n",
    "\n",
    "En este apartado se introducirá el uso de word embeddings para analizar e identificar relaciones semánticas entre palabras para mejorar tareas como la clasificación de textos, la búsqueda de información o la generación de contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se explorará el uso de embeddings de palabras para analizar relaciones semánticas en textos extraídos de comentarios de usuarios de un repositorio de Github, \"zigbee2mqtt\". El estudio se centrará en la identificación de patrones y similutes en las discusiones sobre dispositivos IoT.\n",
    "\n",
    "A través de modelos preentrenados y técnicas de entrenamiento personalizados, se examinará como los embeddings pueden mejorar el análisis semántico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hará uso de la librería *Gensim*, ya conocida y utilizada en los apartados anteriores, para entrenar y utilizar los modelos de embeddings. Gensim proporciona una API eficiente para trabajar con embeddings como Word2Vec, FastText y GloVe, falicitando la implementación de consultas semánticas y visualización de relaciones entre términos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es ***embedding***?\n",
    "Un *embedding* es una representación numérica de un objeto (palabra, frase, etc) en un espacio vectorial de menos dimensión. Se debe diferenciar entre word embedding y documento embedding. *Word embedding* es una representación vectorial de una única palabra, mientras que *document embedding* es una representación vectorial de un documento, es decir, una secuencia de palabras de cualquier longitud estrictamente mayor a 1. \n",
    "\n",
    "En este apartado únicamente se trabajará con **word embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "El objetivo de un algoritmo de *embedding* es encontrar representaciones vectoriales de palabras en un espacio de dimensión *d*, de forma que palabras con significados similares tengan vectores similares. La dimensión *d* es un hiperparámetro del modelo, generalmente establecido entre 50 y 300.\n",
    "\n",
    "Estas dimensiones no tienen un significado predefinido o interpretable, es el modelo el encargado de aprender relaciones latentes entre las palabras presentes en el texto. Cada dimensión representa una posible relación entre palabras, permitiendo que términos similares tengan valores parecidos en ciertas dimensiones.\n",
    "\n",
    "Tras esta explicación, se podría decir que la idea básica del entrenamiento es: las palabras que aparecen en contextos similares tienen significados similares. A esto se le denomina *hipótesis distribucional* (distributional hypothesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy reasoning with word embeddings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
