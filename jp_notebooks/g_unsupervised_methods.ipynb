{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos no supervisados: Topic Modeling y Clustering\n",
    "Cuándo se está trabajando con un dataset compuesto de una gran cantidad de documentos se quiere saber cuál es el tema del que tratan. Es muy complicado asignar un único tema al corpus debido a qué está compuesto por textos con gran variedad de temas, más en este caso en el que se trabaja con un dataset conformado por comentarios de usuarios. \n",
    "\n",
    "Es en este punto cuando entra en juego el Topic Modeling (Modelado de temas). Este tipo de modelado se utiliza para determinar la estructura global del corpus mediante diversas técnicas estadísticas, no para asignar un tema concreto a cada uno de los documentos del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se estudiarán varios métodos para el modelado de temas que permitirán entender su funcionamiento y cómo pueden ser utilizados para generar resúmenes de los documentos de forma rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta tarea se utilizará el dataset conformado por los comentarios de usuarios del repositorio \"Zigbee2mqtt\" que ya ha sido utilizado en tareas anteriores, lo que permite agilizar el proceso de modelado de temas porque este ya está tokenizado, vectorizado, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en todos los apartados, se comienza importando los ajustes del proyecto junto con la base de datos en la que se encuentra almacenado el Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gráficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# # to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# # otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'user', 'text', 'impurity', 'clean_text', 'normalized_text',\n",
      "       'tokens'],\n",
      "      dtype='object')\n",
      "                                                                                                                                                                                           normalized_text  \\\n",
      "0                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
      "1  Also, after updating the z2m, cyclic reboots began ''' Starting Zigbee2MQTT without watchdog. INFO: Preparing to start... INFO: Socat not enabled INFO: Starting Zigbee2MQTT... Starting Zigbee2MQTT...   \n",
      "2  Hi ! Since 2 or 3 days, MQTT suddenly fail. A few messages in the log, many auto restart, and works again ... Very strange. In the log INFO: Preparing to start... ERROR: Got unexpected response fr...   \n",
      "3  I don't know if it's exactly the same, but since v1.42 I have trouble with Z2M. It restarts x times a day without further notice. I think it is a software issue, becaue in older versions I didn't ...   \n",
      "\n",
      "                                                                                                                                                                                                    tokens  \n",
      "0                                                                     This,issue,is,stale,because,it,has,been,open,30,days,with,no,activity,Remove,stale,label,or,comment,or,this,will,be,closed,in,7,days  \n",
      "1  Also,after,updating,the,z2m,cyclic,reboots,began,Starting,Zigbee2MQTT,without,watchdog,INFO,Preparing,to,start,INFO,Socat,not,enabled,INFO,Starting,Zigbee2MQTT,Starting,Zigbee2MQTT,without,watchdo...  \n",
      "2  Hi,Since,2,or,3,days,MQTT,suddenly,fail,A,few,messages,in,the,log,many,auto,restart,and,works,again,Very,strange,In,the,log,INFO,Preparing,to,start,ERROR,Got,unexpected,response,from,the,API,Syste...  \n",
      "3  I,don't,know,if,it's,exactly,the,same,but,since,v1.42,I,have,trouble,with,Z2M,It,restarts,x,times,a,day,without,further,notice,I,think,it,is,a,software,issue,becaue,in,older,versions,I,didn't,see,...  \n"
     ]
    }
   ],
   "source": [
    "#Conexión con la base de datos en la que tenemos guardado el Data Frame\n",
    "db_name = \"../data/zigbee2mqtt_comments.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from comments\", con)\n",
    "con.close()\n",
    "\n",
    "#Comprobación de que se ha cargado correctamente\n",
    "print(df.columns)\n",
    "print(df[['normalized_text', 'tokens']].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recordar que en la columna \"normalized_text\" se encuentran todos los comentarios ya normalizados y vectorizados, es decir, con todos los pasos que se han seguido para obtener un texto limpio y listo para utilizarse en tareas de modelado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos previos\n",
    "Antes de comenzar el modelado, es recomendable conocer la información del corpues para así determinar cuáles son las entidades que se analizarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2678 entries, 0 to 2677\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               2678 non-null   int64  \n",
      " 1   user             2678 non-null   object \n",
      " 2   text             2678 non-null   object \n",
      " 3   impurity         2678 non-null   float64\n",
      " 4   clean_text       2678 non-null   object \n",
      " 5   normalized_text  2678 non-null   object \n",
      " 6   tokens           2678 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 146.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori parece una buena base para el modelado teniendo en cuenta que no hay elemenos nulos en ninguna columna. De todos modos, se puede imprimir por pantalla alguna muestra de estos tectos para comprobar si estos contienen caracteres especiales como puede ser la tabulación (\\t), nueva línea (\\r), retorno de carro (\\r), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days'\n",
      "\"Is thist error? I have (old config) ''' socat: restartdelay: 1 initialdelay: 1 ''' in config but when i check default config this positions will disaapear.\"\n"
     ]
    }
   ],
   "source": [
    "print(repr(df.iloc[0][\"normalized_text\"][0:200]))\n",
    "print(repr(df.iloc[-1][\"normalized_text\"][0:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el data frame ya había sido procesado y limpiado previamente, no se encuentran caracteres especiales que puedan dificultar el modelado.\n",
    "\n",
    "Una vez completado este paso, se pasaría a la vectorización de los datos. En este caso, los datos ya fueron vectorizados anteriormente en *e_featureEngineering_and_syntactSimilarity.ipynb* de este mismo proyecto, permitiendo ahorrar tiempo y agilizar el proceso para cumplir el objetivo de este apartado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorización de matrices no negativas (NMF)\n",
    "Conceptualmente, la forma más sencilla de hallar la estructura implícita en el corpus es la factorización de la matriz de términos, pero puede presentar un gasto computacional muy elevado. \n",
    "\n",
    "En lugar de esto, se puede realizar una factorización aproximada que es menos costoso y al mismo tiempo arroja buenos resultados.\n",
    "\n",
    "Algunos métodos de álgebra lineal permiten representar la matriz como el producto de otras dos matrices no negativas. En este caso se nombrará la matriz original como *V*, y los factores *W* y *H*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de Modelos temáticos usando NMF\n",
    "Casi todos los modelados de temas necesitan un número de temas como parámetro de entrada. En lugar de utilizar todos los temas de todos los textos, se utilizará un número aleatorio para esta tarea, por ejemplo 10. Este número es variable en cualquier caso, al fin y al cabo lo que se busca es un resultado más afinado, pero tampoco puede ser un número demasiado elevado de forma que el gasto computacional exceda los valores deseados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la vectorización de los textos fue llevada a cabo en otro documento del proyecto, en este se deberá realizar el proceso otra vez para poder hacer uso de la variable que almacena los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2678, 1721)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['normalized_text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 2678 entries, 0 to 2677\n",
      "Series name: normalized_text\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "2678 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 21.0+ KB\n",
      "1200                                                            This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days\n",
      "1258                                                                                                                                                                                           !image\n",
      "172                                                                                                                                                 Hi. I have the same problem. Is there a solution?\n",
      "2303                                                                                                                                       Same issue on latest HA with Zigbee2mqtt version: 1.18.1-1\n",
      "914     I had the same \"undefined\" error. In my case it was actually the cheap usb charger I have used for my Raspberry Pi. After switching to a proper power supply everything started successfully.\n",
      "Name: normalized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Comprobación de que no se pierde información\n",
    "df['normalized_text'].info()\n",
    "print(df['normalized_text'].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tema de un texto viene dado por la distribución de las palabras que contiene, por lo tanto, analizar esta distribución puede ser de gran ayuda para descubrir los temas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de la matriz H, se debe encontrar el índice de los mayores valores de cada fila para luego ser utilizado como índice de búsqueda en el vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definirá una función que muestre por pantalla un resumen de los temas (topics) que NMF ha detectado en los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  stale (18.06)\n",
      "  days (17.31)\n",
      "  activity (8.96)\n",
      "  label (8.96)\n",
      "  comment (8.86)\n",
      "\n",
      "Topic 01\n",
      "  add (2.32)\n",
      "  z2m (2.01)\n",
      "  version (1.86)\n",
      "  ha (1.43)\n",
      "  new (1.23)\n",
      "\n",
      "Topic 02\n",
      "  thanks (50.82)\n",
      "  worked (3.38)\n",
      "  working (1.87)\n",
      "  lot (1.09)\n",
      "  reply (0.99)\n",
      "\n",
      "Topic 03\n",
      "  issue (41.26)\n",
      "  having (2.15)\n",
      "  got (1.62)\n",
      "  close (1.11)\n",
      "  exact (1.07)\n",
      "\n",
      "Topic 04\n",
      "  _url_ (29.25)\n",
      "  duplicate (4.55)\n",
      "  related (2.59)\n",
      "  closing (1.28)\n",
      "  device (1.22)\n",
      "\n",
      "Topic 05\n",
      "  error (3.03)\n",
      "  zigbee (2.55)\n",
      "  zigbee2mqtt (2.52)\n",
      "  herdsman (2.11)\n",
      "  info (2.04)\n",
      "\n",
      "Topic 06\n",
      "  problem (21.86)\n",
      "  solution (3.05)\n",
      "  solved (2.78)\n",
      "  zha (1.10)\n",
      "  thank (1.09)\n",
      "\n",
      "Topic 07\n",
      "  config (4.15)\n",
      "  zigbee2mqtt (2.28)\n",
      "  configuration (2.24)\n",
      "  yaml (2.03)\n",
      "  addon (1.85)\n",
      "\n",
      "Topic 08\n",
      "  fixed (16.32)\n",
      "  edge (5.29)\n",
      "  dev (4.58)\n",
      "  branch (4.31)\n",
      "  latest (3.39)\n",
      "\n",
      "Topic 09\n",
      "  update (13.43)\n",
      "  42 (3.94)\n",
      "  working (1.91)\n",
      "  version (1.40)\n",
      "  updated (1.36)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida muestra las palabras más relevantes para cada tema, permitiendo determinar la temática de algunos de los comentarios. Por ejemplo, en *Topic 02* se puede deducir que se trata de un mensaje de agradeciemiento como respuesta a otro y en *Topic 04* que se trata de algún problema con una URL concreta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sería interesante conocer como de grande es una temática, es decir, cuantos comentarios están relacionados con un mismo tema por ejemplo. \n",
    "\n",
    "Esto se puede calcular utilizando la matriz de temas de un documento y sumando las contribuciones individuales a este a lo largo de todos los documentos del data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.15077823, 11.61180167,  5.36082748,  9.99092406,  8.43365604,\n",
       "       10.84861004,  7.45875981, 12.96782206,  6.9400853 ,  8.23673532])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este resultado indica que hay temas de mayor y menor peso pero no hay grandes diferencias en los porcentajes, indicando una supuesta buena calidad al tener una distribución bastante similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
