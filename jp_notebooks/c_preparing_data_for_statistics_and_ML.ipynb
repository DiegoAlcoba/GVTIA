{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos para análisis y Machine Learning\n",
    "En este notebook crearemos un *pipeline* de preprocesamiento de texto similar al visto al principio del estudio, pero más avanzado y haciendo uso de librerías como *spaCy* y *textacy*. Una vez completado, se obtendrá un texto limpio y tokenizado listo para su análisis.\n",
    "\n",
    "Para este caso, se va a hacer uso del dataset creado en el apartado anterior, con más de 2000 comentarios del repoositorio *zigbee2mqtt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en los cuadernos anteriores, comenzaremos cargando unos ajustes predefinidos para la ejecución del entorno virtual de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gráficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecución\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos en Pandas\n",
    "Cargaremos el dataset creado anteriormente con todos los comentarios de un repositorio de Github en Pandas, concretamente el archivo .csv (hay dos idénticos, uno en formato .csv y otro en .json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ruta del archivo\n",
    "file_path = \"../data/output.csv\"\n",
    "\n",
    "#Carga del archivo en un DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar a trabajar con los datos, revisaremos el nombre de las columnas y se cambiarán por otros nombres más genéricos en caso de considerarse necesario para una mejor coprensión y maniobrabilidad con el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'html_url', 'issue_url', 'id', 'node_id', 'user', 'created_at',\n",
      "       'updated_at', 'author_association', 'body', 'reactions',\n",
      "       'performed_via_github_app'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el renombramiento de las columnas, definiremos un diccionario *column_mapping* en el que cada entrada corresponderá con el nombre de la columna original y el nuevo que se le dará. \n",
    "\n",
    "Si se considera que algunas columnas no son necesarias para el análisis, se pueden descartar nombrándolas como *None* o directamente sin incluirlas en el diccionario.\n",
    "\n",
    "Viendo las columnas con las que cuenta el DataFrame se ve a simple vista que hay algunas columnas irrelevantes para el estudio, como las URLs, node_id, fechas de creación y actualización del post, asociaciones y la columna \"performed_via_github_app\". Estas serán descartadas a continuación sin incluirlas en el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2301</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>886619925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>{'login': 'ciotlosm', 'id': 7738048, 'node_id': 'MDQ6VXNlcjc3MzgwNDg=', 'avatar_url': 'https://private-avatars.githubusercontent.com/u/7738048?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>I was away. I will look at this today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         2301\n",
       "id                                                                                                                                                                                                  886619925\n",
       "user  {'login': 'ciotlosm', 'id': 7738048, 'node_id': 'MDQ6VXNlcjc3MzgwNDg=', 'avatar_url': 'https://private-avatars.githubusercontent.com/u/7738048?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJn...\n",
       "text                                                                                                                                                                    I was away. I will look at this today"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast #Para convertir cadenas JSON en objetos Python para la eliminación de campos innecesarios del campo 'user'\n",
    "\n",
    "column_mapping = {\n",
    "    'id' : 'id',\n",
    "    'user' : 'user',\n",
    "    'body' : 'text',\n",
    "    'reactions' : None,\n",
    "    'url' : None,\n",
    "    'html_url' : None,\n",
    "    'issue_url' : None,\n",
    "    'node_id' : None,\n",
    "    'created_at' : None,\n",
    "    'update_at' : None,\n",
    "    'author_association' : None,\n",
    "    'performed_via_github_app' : None\n",
    "}\n",
    "\n",
    "#Se definen las columnas que se mantendrán\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "\n",
    "#Seleccionar y renombrar las columnas\n",
    "df = df[columns].rename(columns=column_mapping)\n",
    "\n",
    "#Normalizamos la columna user para extraer únicamente la información que interesa\n",
    "# user_data = pd.json_normalize(df['user'])\n",
    "\n",
    "# #Asegurar que las columnas que nos interesan existen\n",
    "# if 'login' in user_data.columns and 'id' in user_data.columns:\n",
    "#     df[['login', 'id']] = user_data[['login', 'id']]\n",
    "# else:\n",
    "#     raise ValueError(\"Las columnas 'login' e 'id' no se encuentran en los datos normalizados de 'user'\")\n",
    "\n",
    "# #Se elimina la columna 'user' original\n",
    "# df = df.drop(columns=['user'])\n",
    "\n",
    "#Muestra de una entrada para comprobar que se ha ejecutado correctamente\n",
    "df.sample(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado y carga de un Data Frame\n",
    "Para guardar el Data Frame en disco se hará uso de una base de datos SQL utilizando SQLite. No es necesario contar con conocimientos avanzados de SQL pues se hará uso de la librería *sqlite3* de python que integra todas las funciones necesarias para trabajar con este tipo de bases de datos, como mucho, se usarán sentencias SQL básicas para realizar acciones sobre la base de datos..\n",
    "\n",
    "Cuando guardamos el Data Frame en la base de datos, no se almacena el índice del Data Frame, y todos los datos ya existentes son sobreescritos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sqlite3 ya está importado en el archivo settings.py cargado al iniciar el programa\n",
    "\n",
    "#Damos nombre a la DB, nos conectamos a ella, guardamos los datos, y se cierra conexión\n",
    "db_name = \"../data/zigbee2mqtt_comments.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"comments\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Data Frame se lee de forma muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from comments\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos\n",
    "Antes de la tokenización de los datos es necesario limpiar los datos recopilados de ruido innecesario y distintos formatos en el texto. Algunos ejemplos de estos pueden ser los caracteres especiales, URLs incluidas en los comentarios, etiquetas, emoticonos, etc.\n",
    "\n",
    "Para esta función se usarán expresiones regulares junto con la librería *regex* para detectar y eliminar todos estos elementos innecesarios para el posterior análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re -> importado en settings.py\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]') #símbolos sospechosos de introducir ruido\n",
    "\n",
    "def impurity(text, min_len=10): #se ignoran textos de menos de 10 caracteres\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se procederá a depurar y eliminar el ruido en los comentarios de los posts extraídos del repositorio con el que hemos estado trabajando hasta este momento,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 108613.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>&gt; If I put this config in my zigbee2mqtt 1.25.0-1 and also the below in the config folder it doesn't work. \\n&gt; \\n&gt; \\n&gt; \\n&gt; data_path: /config/zigbee2mqtt\\n&gt; \\n&gt; socat:\\n&gt; \\n&gt;   enabled: false\\n&gt; \\...</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>Sure, done in #307.</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>&gt; &gt; Thanks mate. Gonna check this tomorrow in the afternoon to (hopefully) help you out. I'm now gonna take a nap.\\n&gt; \\n&gt; \\n&gt; \\n&gt; Mate i found the problem.\\n&gt; \\n&gt; if you reset to default values, t...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>&gt; It seems to me that within the configuration tab of the 'Zigbee2MQTT' addon the following lines need to be present (and the rest removed):\\n&gt; \\n&gt; ```\\n&gt; \\n&gt; data_path: /config/zigbee2mqtt\\n&gt; \\n&gt;...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>That works and shows correct state in HA.\\r\\n\\r\\nJust FYI - I forgot to mention before that `{{ value_json.tilt }}` needs to be \"{{ value_json.tilt }}\" to work.</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text  \\\n",
       "1877  > If I put this config in my zigbee2mqtt 1.25.0-1 and also the below in the config folder it doesn't work. \\n> \\n> \\n> \\n> data_path: /config/zigbee2mqtt\\n> \\n> socat:\\n> \\n>   enabled: false\\n> \\...   \n",
       "1902                                                                                                                                                                                      Sure, done in #307.   \n",
       "1866  > > Thanks mate. Gonna check this tomorrow in the afternoon to (hopefully) help you out. I'm now gonna take a nap.\\n> \\n> \\n> \\n> Mate i found the problem.\\n> \\n> if you reset to default values, t...   \n",
       "1884  > It seems to me that within the configuration tab of the 'Zigbee2MQTT' addon the following lines need to be present (and the rest removed):\\n> \\n> ```\\n> \\n> data_path: /config/zigbee2mqtt\\n> \\n>...   \n",
       "1446                                         That works and shows correct state in HA.\\r\\n\\r\\nJust FYI - I forgot to mention before that `{{ value_json.tilt }}` needs to be \"{{ value_json.tilt }}\" to work.   \n",
       "\n",
       "      impurity  \n",
       "1877      0.08  \n",
       "1902      0.05  \n",
       "1866      0.05  \n",
       "1884      0.05  \n",
       "1446      0.05  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se añade la columna \"impurity\" al Data Frame que mostrará el porcentaje de cada comentario\n",
    "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
    "\n",
    "#Algunas muestras de los registros con más ruido\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, el grado de impurezas no es demasiado elevado, pero si se omiten facilitará el trabajo el análisis, ademaś de que siempre se tiene que tener en cuenta debido a que se está trabajando con contenido generado por usuarios, y este puede ser un caso excepcional en el que no hay demasiado ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de otras posibles palabras que pueden introducir ruido\n",
    "Se importará la función *count_words* utilizada en *1-textual_data* para realizar el conteo de palabras de otras etiquetas que no se han tenido en cuenta y que también pueden introducir ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 182740.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;anonymous&gt;</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;redacted&gt;</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/details&gt;</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;details&gt;</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/summary&gt;</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;summary&gt;</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;REDACTED&gt;</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;template&gt;</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/script&gt;</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;snip&gt;</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             freq\n",
       "token            \n",
       "<anonymous>    51\n",
       "<redacted>      5\n",
       "</details>      5\n",
       "<details>       5\n",
       "</summary>      5\n",
       "<summary>       5\n",
       "<REDACTED>      4\n",
       "<template>      4\n",
       "</script>       4\n",
       "<snip>          4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    #procesa los tokens y actualiza el contador\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    #crea el contador y recorre todos los datos\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    #transforma el contador a dataframe\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de ruido con Expresiones Regulares\n",
    "Se va a crear una función que definirá una serie de expresiones regulares que serán utilizadas para detectar en el texto una serie de patrones que cumplen aquellas palabras susceptibles de introducir ruido. Estas serán sustituidas por texto plano o eliminadas directamente del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) \n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se aplicará esta función a la columna \"text\" del Data Frame que almacena los comentarios de los usuarios en el repositorio, además, se añadirá una nueva columna con el texto limpio, de modo que se pueda visualizar más fácilmente los cambios entre el texto original y el texto sin ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 14116.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                         text  \\\n",
      "2440  After browsing the debug logs, it seems that MQTT is the bottleneck. At least it takes seconds, after a PUBLISH is sent until it is received from the zigbee2mqtt component. Of course it could also...   \n",
      "305                                                                                             I confirm as well, I commented out availabilty section in the config and the interface is now working again.    \n",
      "226                                                                                                                                                                           I also have this issue.\\r\\n\\r\\n   \n",
      "1121                                                        Hi, had the same yesterday, tried CLI command for restarting and repairing  supervisor, didn't help. Restarted VM with HA and problem was solved.   \n",
      "1533  \\n> @Ricardo-0101 your config looks wrong to me, under the serial section you have to fill in just:\\n> \\n> \\n> \\n> ```yaml\\n> \\n> port: tcp://tubeszb-cc2652-poe-2022.local:6638\\n> \\n> ```\\n\\n@Koen...   \n",
      "\n",
      "                                                                                                                                                                                                   clean_text  \n",
      "2440  After browsing the debug logs, it seems that MQTT is the bottleneck. At least it takes seconds, after a PUBLISH is sent until it is received from the zigbee2mqtt component. Of course it could also...  \n",
      "305                                                                                              I confirm as well, I commented out availabilty section in the config and the interface is now working again.  \n",
      "226                                                                                                                                                                                   I also have this issue.  \n",
      "1121                                                         Hi, had the same yesterday, tried CLI command for restarting and repairing supervisor, didn't help. Restarted VM with HA and problem was solved.  \n",
      "1533  @Ricardo-0101 your config looks wrong to me, under the serial section you have to fill in just: ```yaml port: tcp://tubeszb-cc2652-poe-2022.local:6638 ``` @Koenkk You are exactly right, can't beli...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].progress_apply(clean)\n",
    "\n",
    "#Muestras de la columna \"clean_text\"\n",
    "print(df[['text', 'clean_text']].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya se había indicado antes que con suerte la información extraída no contenía demasiado ruido, pese a eso, se puede ver a simple vista la diferencia entre algunos textos originales y los textos ya sin ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de caracteres con *textacy*\n",
    "Caracteres especiales como los acentos, apóstrofes, diéresis, etc. pueden ser un problema a la hora de tokenizar un texto, por ello se normalizará sustituyendo estos caracteres por equivalentes ASCII para evitar así inconvenientes.\n",
    "\n",
    "Se utilizará la librería *textacy* creada para trabajar junto con *spaCy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enmascaramiento de datos basados en patrones\n",
    "Del mismo modo, hay patrones como URLs y correos electrónicos que habitualmente tampoco serán de ayuda para el análisis de la información, es por ello que estos patrones se pueden sustituir/enmascarar por un texto simple en lugar de eliminarlo del texto porque cabe la posibilidad de perder el contexto de la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "# En caso de que se cuente con una versión menor a la 0.11\n",
    "if textacy.__version__ < '0.11':\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize_hyphenated_words(text)\n",
    "        text = tprep.normalize_quotation_marks(text)\n",
    "        text = tprep.normaliza_unicode(text)\n",
    "        text = tprep.remove_accents(text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "else:\n",
    "    #En mi caso cuento con la versión 0.13\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize.hyphenated_words(text)\n",
    "        text = tprep.normalize.quotation_marks(text)\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        #Enmascaramiento de patrones\n",
    "        text = tprep.replace.urls(text)\n",
    "        text = tprep.replace.emails(text)\n",
    "        text = tprep.replace.emojis(text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se aplicará la normalización sobre el texto limpio sin ruido obtenido en el apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:01<00:00, 2255.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                         text  \\\n",
      "503   Can we get an update on this issue please? It has been plaguing me for months and after multiple updates still not fixed.\\r\\n\\r\\nI have ended up implementing automation to monitor multiple devices...   \n",
      "1932  Add me to the list, I can't for the life of me get Zigbee2mqtt to work in HA, I have been running Domoticz and Zigbee2mqtt for around 2 years, never had an issue there.\\r\\n\\r\\nI got curious about ...   \n",
      "900   @Koenkk nailed it ... erased and reflashed coordinator firmware to my cc2531 and it is cooperating now. \\r\\n\\r\\nNow I just need to figure out how to \"import\" the existing devices that are paired w...   \n",
      "2302                                                                                                                           @ciotlosm I'm noticing many users do not see the update, could you check this?   \n",
      "1593                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
      "\n",
      "                                                                                                                                                                                                   clean_text  \\\n",
      "503   Can we get an update on this issue please? It has been plaguing me for months and after multiple updates still not fixed. I have ended up implementing automation to monitor multiple devices around...   \n",
      "1932  Add me to the list, I can't for the life of me get Zigbee2mqtt to work in HA, I have been running Domoticz and Zigbee2mqtt for around 2 years, never had an issue there. I got curious about HA so I...   \n",
      "900   @Koenkk nailed it ... erased and reflashed coordinator firmware to my cc2531 and it is cooperating now. Now I just need to figure out how to \"import\" the existing devices that are paired with it i...   \n",
      "2302                                                                                                                           @ciotlosm I'm noticing many users do not see the update, could you check this?   \n",
      "1593                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
      "\n",
      "                                                                                                                                                                                              normalized_text  \n",
      "503   Can we get an update on this issue please? It has been plaguing me for months and after multiple updates still not fixed. I have ended up implementing automation to monitor multiple devices around...  \n",
      "1932  Add me to the list, I can't for the life of me get Zigbee2mqtt to work in HA, I have been running Domoticz and Zigbee2mqtt for around 2 years, never had an issue there. I got curious about HA so I...  \n",
      "900   @Koenkk nailed it ... erased and reflashed coordinator firmware to my cc2531 and it is cooperating now. Now I just need to figure out how to \"import\" the existing devices that are paired with it i...  \n",
      "2302                                                                                                                           @ciotlosm I'm noticing many users do not see the update, could you check this?  \n",
      "1593                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['normalized_text'] = df['clean_text'].progress_apply(normalize)\n",
    "\n",
    "#Muestras de la columna \"clean_text\"\n",
    "print(df[['text', 'clean_text', 'normalized_text']].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras haber obtenido un texto limpio, se realiza la conexión a la base de datos para guardar los cambios realizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"comments\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "Como ya se explicó en el primer capítulo del estudio, vamos a tokenizar la información que tenemos para facilitar así su análisis.\n",
    "\n",
    "En este apartado se van a ver dos versiones distintas, una utilizando *expresiones regulares* (similar al visto en el primer capítulo) y otra con la librería *NLTK*.\n",
    "\n",
    "Como lo que se va a tokenizar no es un texto simple, si no todas las entradas de la columna *\"normalized_text\"*, se creará una función que reciba como parámetro un texto que corresponderá con cada entrada de la columna, lo tokenizará y lo devolverá, para a continuación guardarlo en otra columna llamada *\"tokens\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización con Expresiones Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 38833.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                             normalized_text  \\\n",
      "1434                                                                                    You can remove this option, it can now be configured via the z2m frontend (see _URL_   \n",
      "1843                                    It looks database.db file has some problem. I want to open the file and edit it. But, I cannot open it. Is there the way to edit it?   \n",
      "1817  I have the same problem. I restored 1.25.0.1 from backup last night and tried again this morning with no success. Fixed by clearing cache on Chrome and on android App   \n",
      "\n",
      "                                                                                                                                                                                       tokens  \n",
      "1434                                                                                      [You, can, remove, this, option, it, can, now, be, configured, via, the, z2m, frontend, see, _URL_]  \n",
      "1843                                  [It, looks, database, db, file, has, some, problem, want, to, open, the, file, and, edit, it, But, cannot, open, it, Is, there, the, way, to, edit, it]  \n",
      "1817  [have, the, same, problem, restored, 25, from, backup, last, night, and, tried, again, this, morning, with, no, success, Fixed, by, clearing, cache, on, Chrome, and, on, android, App]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Función tokenizadora\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\w\\w+', text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#Tokenización de cada entrada del Data Frame\n",
    "df['tokens'] = df['normalized_text'].progress_apply(tokenize)\n",
    "\n",
    "#Impresión por pantalla de algunas muestras de entradas tokenizadas\n",
    "print(df[['normalized_text', 'tokens']].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se se observa que algunas expresiones como caracteres especiales o emojis se han perdido, se puede modificar la función para incluir tipos de expresiones que se desean incluir en la tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 24234.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                              normalized_text  \\\n",
      "1751                                                                                                                                                            If the frontend is up you've fixed it _EMOJI_   \n",
      "2278                                                        @mdegat01 Can you please update this for latest version with changelog and version update please? I have been away and missed quite a few updates   \n",
      "1428  ug 2022-08-27 23:26:52Received Zigbee message from 'YoolaxTest', type 'attributeReport', cluster 'closuresWindowCovering', data '{\"currentPositionLiftPercentage\":92}' from endpoint 1 with groupID ...   \n",
      "\n",
      "                                                                                                                                                                                                       tokens  \n",
      "1751                                                                                                                                                  [If, the, frontend, is, up, you've, fixed, it, _EMOJI_]  \n",
      "2278                               [@mdegat01, Can, you, please, update, this, for, latest, version, with, changelog, and, version, update, please, I, have, been, away, and, missed, quite, a, few, updates]  \n",
      "1428  [ug, 2022-08-27, 23:26:52Received, Zigbee, message, from, 'YoolaxTest, type, 'attributeReport, cluster, 'closuresWindowCovering, data, currentPositionLiftPercentage, :92, from, endpoint, 1, with, ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RE_TOKEN = re.compile(r\"\"\"\n",
    "               ( [#]?[@\\w'’\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
    "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
    "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
    "               )\n",
    "               \"\"\", re.VERBOSE)\n",
    "\n",
    "def ttokenize(text):\n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "df['tokens'] = df['normalized_text'].progress_apply(ttokenize)\n",
    "\n",
    "print(df[['normalized_text', 'tokens']].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tras la tokenización, en la columna \"tokens\" se almacena una lista de palabras (tokens)\n",
    "#Como sqlite no sabe como manejas valores de tipo lista, hay que transformar estos en texto\n",
    "df['tokens'] = df['tokens'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "#Guarda el Data Frame en la base de datos\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"comments\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización con biblioteca NLTK\n",
    "El resultado será similar al obtenido en el apartado anterior (independientemente de la biblioteca utilizada, al fin y al cabo). El usuario es el que decide si desea definir las expresionres regulares por su cuenta o utilizar un diccionario ya aportado por una librería.\n",
    "\n",
    "Cabe recalcar que hay librerías con diccionarios muy completos, y estos siempre son accesibles para la modificación por parte del usuario, ya sea para añadir o para eliminar expresiones.\n",
    "\n",
    "En este caso se va a utiliar el módulo *punkt* de NLTK, un paquete que incluye herramientas y datos preentrenados para la tokenización de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'user', 'text', 'impurity', 'clean_text', 'normalized_text',\n",
      "       'tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/diego/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/diego/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:01<00:00, 2457.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                           normalized_text  \\\n",
      "0                                                                    This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 7 days   \n",
      "1  Also, after updating the z2m, cyclic reboots began ''' Starting Zigbee2MQTT without watchdog. INFO: Preparing to start... INFO: Socat not enabled INFO: Starting Zigbee2MQTT... Starting Zigbee2MQTT...   \n",
      "2  Hi ! Since 2 or 3 days, MQTT suddenly fail. A few messages in the log, many auto restart, and works again ... Very strange. In the log INFO: Preparing to start... ERROR: Got unexpected response fr...   \n",
      "3  I don't know if it's exactly the same, but since v1.42 I have trouble with Z2M. It restarts x times a day without further notice. I think it is a software issue, becaue in older versions I didn't ...   \n",
      "4  Hello, I have the same problem, see the log file zigbee2mqtt_2024-12-17T07-07-00.298Z.log for details. Everything runs normally until 00:37:59, after which no more data is received. At 03:00:06 th...   \n",
      "\n",
      "                                                                                                                                                                                                    tokens  \n",
      "0                                      [This, issue, is, stale, because, it, has, been, open, 30, days, with, no, activity, ., Remove, stale, label, or, comment, or, this, will, be, closed, in, 7, days]  \n",
      "1  [Also, ,, after, updating, the, z2m, ,, cyclic, reboots, began, ``, ', Starting, Zigbee2MQTT, without, watchdog, ., INFO, :, Preparing, to, start, ..., INFO, :, Socat, not, enabled, INFO, :, Start...  \n",
      "2  [Hi, !, Since, 2, or, 3, days, ,, MQTT, suddenly, fail, ., A, few, messages, in, the, log, ,, many, auto, restart, ,, and, works, again, ..., Very, strange, ., In, the, log, INFO, :, Preparing, to...  \n",
      "3  [I, do, n't, know, if, it, 's, exactly, the, same, ,, but, since, v1.42, I, have, trouble, with, Z2M, ., It, restarts, x, times, a, day, without, further, notice, ., I, think, it, is, a, software,...  \n",
      "4  [Hello, ,, I, have, the, same, problem, ,, see, the, log, file, zigbee2mqtt_2024-12-17T07-07-00.298Z.log, for, details, ., Everything, runs, normally, until, 00:37:59, ,, after, which, no, more, d...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas() #Inicializa el soporte para Pandas\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#Función para tokenizar las columnas indicadas del Data Frame con NLTK\n",
    "def nltk_tokenize(text):\n",
    "    try:\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error al tokenizar: {e}\")\n",
    "        return []\n",
    "\n",
    "# # Rellenar valores NaN con una cadena vacía antes de tokenizar\n",
    "df['normalized_text'] = df['normalized_text'].fillna('')\n",
    "\n",
    "df['tokens'] = df['normalized_text'].progress_apply(nltk_tokenize)\n",
    "\n",
    "print(df[['normalized_text', 'tokens']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
