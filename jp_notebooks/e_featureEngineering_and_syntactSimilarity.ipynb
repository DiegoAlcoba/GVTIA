{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de características y Similitud sintáctica "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se verá principalmente cómo *vectorizar* documentos, que consiste en convertir texto no estructurado en vectores compuestos por números.\n",
    "\n",
    "Debido a que la vectorización es la base para casi todas las tareas de *Machine Learning*, en este capítulo se trabajará con dos modelos dispuestos por la librería *scikit-learn*, además de construir nuestro propio vectorizador, útil para futuros proyectos por ser ajustable a las tareas que en cada momento el usuario considere necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Carga del archivo setup.py\n",
    "%run -i ../pyenv_settings/setup.py\n",
    "\n",
    "#Imports y configuraciones de gráficas\n",
    "%run \"$BASE_DIR/pyenv_settings/settings.py\"\n",
    "\n",
    "#Reset del entorno virtual al iniciar la ejecución\n",
    "#%reset -f\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# # to print output of all statements and not just the last\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# # otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "# pd.set_option('display.html.use_mathjax', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras cargar los ajustes y preferencias del entorno virtual, podemos proceder a crear nuestro primer vectorizador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de un Vectorizador\n",
    "Se va a construir un vectorizador que se usará sobre el Data Frame con el que se ha estado trabajando en los capítulos anteriores. Como primer paso, se deberá cargar el dataset guardado en la base de datos, cuya contenido (los comentarios de los usuarios) ya está normalizado y tokenizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexión con la base de datos en la que tenemos guardado el Data Frame\n",
    "db_name = \"../data/zigbee2mqtt_comments.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from comments\", con)\n",
    "con.close()\n",
    "\n",
    "#Comprobación de que se ha cargado correctamente\n",
    "print(df.columns)\n",
    "print(df[['normalized_text', 'tokens']].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumeración del vocabulario\n",
    "En primer lugar se van a enumerar todas las palabras de los comentarios normalizados del Data Frame (tokens), de este modo para referirnos a una palabra específica, se puede usar su número asociado (índice del diccionario generado) para crear los vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Antes que nada*, recordar que los tokens se almacenan en el Data Frame como palabras separadas por comas, no como una lista de palabras, por ello hay que convertir cada entrada de la columna \"tokens\" a listas que tengan cada palabra como una única entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asegurarse de que la columna 'tokens' contenga listas y no cadenas de texto\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x.split(','))\n",
    "\n",
    "#Verificar cómo se ve ahora la columna 'tokens'\n",
    "print(df['tokens'].head(4))\n",
    "\n",
    "#Aunque parezca que no ha cambiado, ahora es precisamente una lista y se podrá crear\n",
    "#el diccionario correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion del diccionario a partir de la columna tokens ya existente\n",
    "vocabulary = set([word for tokens in df['tokens'] for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumeración de las palabras (tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impresión del diccionario con sus índices creado\n",
    "# for word, i in word_to_index.items():\n",
    "#     print(f\"'{word}': {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, el diccionario cuenta con un total de 12617 entradas. Ahora se incluirá una nueva columna en el Data Frame en el que se indicará el índice de cada token que aparece en cada entrada de texto (comentario) correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2678 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2678/2678 [00:00<00:00, 62298.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#Se añaden los índices de los tokens a la columna 'token_index' del DF\n",
    "df['token_index'] = df['tokens'].progress_apply(lambda tokens: [word_to_index.get(token, -1) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de documentos\n",
    "Para comparar vectores se debe asegurar que todos cuentan con las mismas dimensiones, por ello se utiliza el mismo diccionario para todos.\n",
    "\n",
    "Si un texto no contiene una palabra, se indica con un 0 en su posición, en caso contrario, se indica con un 1. Se deduce entonces que la longitud de los vectores será igual a la longitud del diccionario generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se definirá una función que codificará todos los textos en vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(text):\n",
    "    return [1 if w in text else 0 for w in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores generados: 2678\n",
      "Total de entradas del Data Frame: 2678\n"
     ]
    }
   ],
   "source": [
    "#Generación de los vectores one-hot\n",
    "onehot_vectors = [onehot_encode(text) for text in df['normalized_text']]\n",
    "\n",
    "#Verificación de que se han codificado todas las entradas\n",
    "print(f\"Total de vectores generados: {len(onehot_vectors)}\")\n",
    "print(f\"Total de entradas del Data Frame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, vector in zip(df['normalized_text'].head(2), onehot_vectors):\n",
    "    print(\"One-hot vector: \")\n",
    "    print(vector)\n",
    "    print(\" - Normalized text: \")\n",
    "    print(text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de términos del documento\n",
    "En la matriz de términos están todos los términos del vocabulario dispuestos en las columnas, cada fila corresponde con cada documento de texto y se indica con 0 o 1 si ese término aparece o no en el documento.\n",
    "\n",
    "Con esta matriz se pierde la posibilidad de calcular la frecuencia con la que aparece una palabra, pero es la construcción más básica que se utilizará para casi todas las tareas relacionadas con Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=list(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de esta matriz es de gran ayuda a la hora de calcular similitudes entre los documentos que se analizan. Esto se consigue calculando el número de 1s en la misma posición entre las distintias entradas.\n",
    "\n",
    "Ahora se mostrará como se calcula la similitud entre las dos primeras entradas del Data Frame que se ha estado usando en el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simil = [onehot_vectors[0][i] & onehot_vectors[1][i] for i in range(0, len(vocabulary))]\n",
    "sum(simil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea calcular la similitud entre todas las entradas del Data Frame, se puede hacer de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "np.dot(onehot_vectors, np.transpose(onehot_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la salida de esta operación se observan distintos arrays, cada uno en una fila, correspondientes con cada entrada del Data Frame.\n",
    "\n",
    "Cada posición del array es la similitud de esa entrada con el resto, por eso los valores más altos de similitud se localizan en la diagonal, pues coincide con la similitud de una entrada con ella misma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización con *scikit learn*\n",
    "Como alternativa, se pueden vectorizar los documentos haciendo uso de la librería scikit-learn, que además proporcionará la posibilidad de calcular la frecuencia de aparición de un símbolo haciendo uso de la representación *bag-of-words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario\n",
    "A continuación, el modelo debe aprenderse el vocabulario perteneciente a todas las entradas del Data Frame que se desea analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(df['normalized_text'])\n",
    "\n",
    "#Visualización por pantalla del vocabulario\n",
    "print(cv.vocabulary_)\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de text a vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = cv.transform(df['normalized_text'])\n",
    "vecs\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función transforma todas las entradas de texto a una matriz dispersa (sólo almacena aquellas posiciones en las que hay un 1) en la que cada fila representa una entrada del Data Frame y cada columna un término del vocabulario.\n",
    "\n",
    "Se transformará una matriz completa para facilitar su lectura en caso de considerarse necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v_matrix = vecs.toarray()\n",
    "v_matrix = pd.DataFrame(vecs.toarray(), columns=cv.get_feature_names_out())\n",
    "print(v_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de similitudes\n",
    "Scikit-learn ofrece una función que permite calcular la similitud entre dos entradas (textos) o la similitud de todo el Data Frame. A continuación se muestra cómo usar esta función para estos dos casos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02243308]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Cálculo de similitud entre dos entradas\n",
    "cosine_similarity(vecs[0], vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo de similitud de todo el Data Frame\n",
    "pd.DataFrame(cosine_similarity(vecs, vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se indicó anteriormente, se observa que en la diagonal los valores son de *1.00* porque se está comparando cada entrada con ella misma, en el resto de posiciones se observa la similitud con el resto de entradas del Data Frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo TF/IDF\n",
    "A la hora de analizar documentos, cabe la posibilidad de que una elevada similitud no aporte demasiada información y sin embargo, aquellos términos que aparecen con menor frecuencia cuenten con mucha.\n",
    "\n",
    "En este punto entra en juego este tipo de modelos, que se encargan de contar el total de apariciones de un símbolo/palabra y así reducir el peso de las palabras con mucha frecuencia y aumentar el de aquellas menos comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo ya se vió anteriormente, una mejor forma de medir la información es calculando la frecuencia inversa del documento.\n",
    "\n",
    "En este caso, el peso TF/IDF puede calculares a partir del modelo bag-of-words desarrollado hace un momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_vecs = tfidf.fit_transform(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_vecs.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se observa como el peso de alguanas palabras que tenían una elevada frecuencia ha disminuido, al mismo tiempo que el peso de aquellas que no contaban con una elevada frecuencia no ha variado demasiado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el efecto que este proceso tiene sobre la matriz de similitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cosine_similarity(tfidf_vecs, tfidf_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia que en muchos casos el valor de la similitud ha descendido al reducirse el peso de aquellas palabras con una elevada frecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitación de tipos de palabras\n",
    "Podemos mejorar la densidad de información que obtendremos limitando los tipos de palabras que queremos mantener en nuestro Data Frame. Como existen tipos de palabras que no aportan demasiada información, como preposiciones o conjugaciones, se pueden obviar y así reducir el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 93810 stored elements and shape (2678, 8443)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz en este momento cuenta con 93.810 elementos almacenados.\n",
    "Veamos ahora cuántos tendrá una vez se haya vectorizado y ajustado la información utilizando palabras de parada como filtro (Una vez más, la columna *\"normalized text\"* del Data Frame ya contenía datos previamente limpiados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_set\n",
    "\n",
    "#Se pasa el set de stop words a lista\n",
    "stopwords = list(stopwords_set)\n",
    "\n",
    "#Crea el vectorizador TF/IDF con stopwords\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "#Función que ajusta y transforma los textos\n",
    "#Funciones anteriores en una única función\n",
    "vecs = tfidf.fit_transform(df['normalized_text'].map(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 59363 stored elements and shape (2678, 8194)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar la transformación, se observa claramente que el número de elementos almacenados ha disminuido hasta los 59.363 elementos, lo que reduce considerablemente la cantidad de elementos que se consideran de importancia para el estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de los elementos más comunes\n",
    "Si un elemento de la matriz cuenta con una frecuencia demasiado elevada, cabe esperar que su aporte no debe de ser de gran importancia, es incluso posible que se trate de una palabra que no se ha incluido como stop word.\n",
    "\n",
    "En este caso no se considera necesario su uso, ya que la reducción de elementos ya ha sido considerable con el uso del diccionario de stop words de spaCy.\n",
    "\n",
    "De todos modos, se va a escribir el código que ejecutaría esta tarea en caso de ser necesario en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt\", header=None)\n",
    "#top_10000_list = list(top_10000)\n",
    "# tfidf = TfidfVectorizer(stop_words=set(top_10000.iloc[:,0].values))\n",
    "# vecs = tfidf.fit_transform(df['normalized_text'].map(str))\n",
    "# vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mayor contexto usando N-Grams\n",
    "Al contrario que en los apartados anteriores, ahora se añadirá contexto a la información incluyendo n-grams de, por ejemplo, 2 dimensiones.\n",
    "\n",
    "Como únicamente se había trabajado con términos de una única palabra, ahora se tendrán en cuenta aquellos compuestos por más de una, cuya utilización está muy extendida sobre todo en el inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2678, 5349)\n",
      "(2678, 8597)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 45015 stored elements and shape (2678, 8597)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = list(set(top_10000[0].values))\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=stopwords, min_df=2)\n",
    "vecs = tfidf.fit_transform(df['normalized_text'].map(str))\n",
    "print(vecs.shape)\n",
    "vecs\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords, min_df=2)\n",
    "vecs = tfidf.fit_transform(df['normalized_text'].map(str))\n",
    "print(vecs.shape)\n",
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pese a que ha disminuido el número de elementos contenidos en la matriz, esto es debido a que todas esos elementos ahora se han unido para crear n-grams de 2 o 3 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras relacionadas\n",
    "A menudo, hay ciertas palabras que suelen aparecer juntas en los textos.\n",
    "\n",
    "En este apartado se tratará de encontrar aquellas palabras que suelen aparecer juntas una cierta cantidad de veces, en este caso, unas 300 veces a lo largo de todo el Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 2035 stored elements and shape (2678, 5)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=300)\n",
    "rel_words = tfidf_word.fit_transform(df['normalized_text'])\n",
    "\n",
    "rel_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado se han obtenido 2.035 elementos de palabras relacionadas usando el diccionario de las 10.000 palabras con más frecuencia de aparición usado en el apartado anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula la similitud en el vector obtenido de palabras relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_freq = cosine_similarity(rel_words.T, rel_words.T)\n",
    "np.fill_diagonal(rel_freq, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stale\" related to \"30\"\n",
      "\"zigbee2mqtt\" related to \"z2m\"\n",
      "\"z2m\" related to \"_url_\"\n",
      "\"zigbee2mqtt\" related to \"30\"\n",
      "\"zigbee2mqtt\" related to \"_url_\"\n",
      "\"z2m\" related to \"30\"\n",
      "\"_url_\" related to \"30\"\n",
      "\"z2m\" related to \"stale\"\n",
      "\"zigbee2mqtt\" related to \"stale\"\n",
      "\"stale\" related to \"_url_\"\n"
     ]
    }
   ],
   "source": [
    "#Para mostrar por pantallas las palabras relacionadas\n",
    "voc = tfidf_word.get_feature_names_out()\n",
    "size = rel_freq.shape[0] # quadratic\n",
    "for index in np.argsort(rel_freq.flatten())[::-1][0:40]:\n",
    "    a = int(index/size)\n",
    "    b = index%size\n",
    "    if a > b:  # avoid repetitions\n",
    "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando el resultado por pantalla, quizás en este caso no pueda ser demasiado significativo ya que el dataset que se está utilizando no es demasiado amplio, pero en un futuro proyecto en el que la cantidad de información sea significativamente mayor puede ser de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las palabras relacionadas a Data Frame\n",
    "related_df = pd.DataFrame(rel_words.toarray(), columns=tfidf_word.get_feature_names_out())\n",
    "\n",
    "#Conexión con la bd y almacenamiento de las palabras \n",
    "con = sqlite3.connect(db_name) \n",
    "related_df.to_sql(\"related_words\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvtiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
